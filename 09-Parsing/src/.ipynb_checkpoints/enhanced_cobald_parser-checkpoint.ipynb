{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9740343-7063-4c1d-95e2-f579c9f7778f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor, BoolTensor, LongTensor\n",
    "import numpy as np\n",
    "\n",
    "# Make cell output in Jupyter notebook scroll horizontally\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "# Right version\n",
    "# !pip install jupyterlab-vim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a27e51-1500-4942-8f8b-55bea06227ba",
   "metadata": {},
   "source": [
    "Enhanced **Co**mpreno **Ba**sed **L**inguistic **D**ata - формат для разметки морфологии, синтаксиса и семантики предложений\n",
    "\n",
    "* Расширяет Enhanced Universal Dependencies путём добавления упрощённой семантической разметки Compreno\n",
    "* Следует стандарту CoNLL-U Plus\n",
    "* Содержит 12 колонок\n",
    "\n",
    "![example](img/EnhancedCobaldStructure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd18094-777a-4748-9caa-60c45e5ca25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sent_id = 13\n",
      "# text = Well doesn't that include rock? \n",
      "1\tWell\twell\tINTJ\tInterjection\t_\t5\tdiscourse\t5:discourse\t_\tParenthetical\tDISCOURSIVE_UNITS\n",
      "2-3\tdoesn't\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "2\tdoes\tdo\tAUX\tVerb\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t5\taux\t5:aux\t_\t_\tAUXILIARY_VERBS\n",
      "3\tn't\tnot\tPART\t_\tPolarity=Neg\t5\tadvmod\t5:advmod\t_\t_\t_\n",
      "4\tthat\tthat\tPRON\tPronoun\tNumber=Sing|PronType=Dem\t5\tnsubj\t4.1:det\t_\tCh_Reference\tCH_REFERENCE_AND_QUANTIFICATION\n",
      "4.1\t#NULL\t#NULL\tNOUN\tNoun\tNumber=Sing\t_\t_\t5:nsubj\tellipsis\tPossessor_Locative\tENTITY_OR_SITUATION_PRONOUN\n",
      "5\tinclude\tinclude\tVERB\tVerb\tVerbForm=Inf\t0\troot\t0:root\t_\tPredicate\tCONTAIN_INCLUDE_FORM\n",
      "6\trock\trock\tNOUN\tNoun\tNumber=Sing\t5\tobj\t5:obj\tSpaceAfter=No\tObject\tDYNAMIC_ARTS\n",
      "7\t?\t?\tPUNCT\tPUNCT\t_\t5\tpunct\t5:punct\t_\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ../data/train_sample.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eda785-3581-4f0b-b38f-5e0fc0ac5abb",
   "metadata": {},
   "source": [
    "Наша цель на сегодня - научиться автоматически аннотировать неразмеченные предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3375e0-7ac9-481f-b6b7-66408e0c46d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sent_id = 110\n",
      "# text = You are kidding, right?\n",
      "1\tYou\t\t\t\t\t\t\t\t\t\t\n",
      "2\tare\t\t\t\t\t\t\t\t\t\t\n",
      "3\tkidding\t\t\t\t\t\t\t\t\t\t\n",
      "4\t,\t\t\t\t\t\t\t\t\t\t\n",
      "5\tright\t\t\t\t\t\t\t\t\t\t\n",
      "6\t?\t\t\t\t\t\t\t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "!cat ../data/test_sample_clean.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115690b6-8946-45c9-8074-ba357257345f",
   "metadata": {},
   "source": [
    "Мы будем решать задачу в постановке sequence tagging (как NER). Точнее, multi-task sequence tagging, когда для каждого токена надо предсказать не один, а фиксированное число тегов N > 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9e882-2340-4066-9bb4-28ccdffa6bd2",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286c23f-8ac1-4519-9351-afb02f4bf14a",
   "metadata": {},
   "source": [
    "Для начала выкидываем \"токены-диапазоны\", чтобы не мешались. Предсказывать для них все равно нечего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05bd29e8-8357-44e8-bc75-d07ac7c68502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load sentences...\n",
      "Processing...\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 26379.27it/s]\n",
      "Writing results\n",
      "Done.\n",
      "# sent_id = 13\n",
      "# text = Well doesn't that include rock?\n",
      "1\tWell\twell\tINTJ\tInterjection\t_\t5\tdiscourse\t5:discourse\t_\tParenthetical\tDISCOURSIVE_UNITS\n",
      "2\tdoes\tdo\tAUX\tVerb\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t5\taux\t5:aux\t_\t_\tAUXILIARY_VERBS\n",
      "3\tn't\tnot\tPART\t_\tPolarity=Neg\t5\tadvmod\t5:advmod\t_\t_\t_\n",
      "4\tthat\tthat\tPRON\tPronoun\tNumber=Sing|PronType=Dem\t5\tnsubj\t4.1:det\t_\tCh_Reference\tCH_REFERENCE_AND_QUANTIFICATION\n",
      "4.1\t#NULL\t#NULL\tNOUN\tNoun\tNumber=Sing\t_\t_\t5:nsubj\tellipsis\tPossessor_Locative\tENTITY_OR_SITUATION_PRONOUN\n",
      "5\tinclude\tinclude\tVERB\tVerb\tVerbForm=Inf\t0\troot\t0:root\t_\tPredicate\tCONTAIN_INCLUDE_FORM\n",
      "6\trock\trock\tNOUN\tNoun\tNumber=Sing\t5\tobj\t5:obj\tSpaceAfter=No\tObject\tDYNAMIC_ARTS\n",
      "7\t?\t?\tPUNCT\tPUNCT\t_\t5\tpunct\t5:punct\t_\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../data/preprocessing.py ../data/train_sample.conllu ../data/train_sample_processed.conllu\n",
    "!cat ../data/train_sample_processed.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d4b5a-174b-457e-b9bd-845c8b3a4c2d",
   "metadata": {},
   "source": [
    "Пара удобных абстракций:\n",
    "\n",
    "* *parse_conllu_incr* парсит за нас conllu формат и возвращает список токенов\n",
    "* С сырыми списками токенов работать неудобно, поэтому *Sentence* берет на себя логику сбора полей токенов в одно поле (и еще по мелочи)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5819fca-c8ff-4952-a977-100e8ada78e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence.words: ['Well', 'does', \"n't\", 'that', '#NULL', 'include', 'rock', '?']\n",
      "sentence.lemmas: ['well', 'do', 'not', 'that', '#NULL', 'include', 'rock', '?']\n",
      "sentence.upos: ['INTJ', 'AUX', 'PART', 'PRON', 'NOUN', 'VERB', 'NOUN', 'PUNCT']\n",
      "sentence.xpos: ['Interjection', 'Verb', '_', 'Pronoun', 'Noun', 'Verb', 'Noun', 'PUNCT']\n",
      "sentence.feats: [{}, {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}, {'Polarity': 'Neg'}, {'Number': 'Sing', 'PronType': 'Dem'}, {'Number': 'Sing'}, {'VerbForm': 'Inf'}, {'Number': 'Sing'}, {}]\n",
      "sentence.heads: [6, 6, 6, 6, -1, 0, 6, 6]\n",
      "sentence.deprels: ['discourse', 'aux', 'advmod', 'nsubj', '_', 'root', 'obj', 'punct']\n",
      "sentence.deps: [{6: 'discourse'}, {6: 'aux'}, {6: 'advmod'}, {5: 'det'}, {6: 'nsubj'}, {0: 'root'}, {6: 'obj'}, {6: 'punct'}]\n",
      "sentence.deps: [{6: 'discourse'}, {6: 'aux'}, {6: 'advmod'}, {5: 'det'}, {6: 'nsubj'}, {0: 'root'}, {6: 'obj'}, {6: 'punct'}]\n"
     ]
    }
   ],
   "source": [
    "from common.parse_conllu import parse_conllu_incr\n",
    "from common.token import Token\n",
    "from common.sentence import Sentence\n",
    "\n",
    "\n",
    "conllu_file_path = \"../data/train_sample_processed.conllu\"\n",
    "\n",
    "sentences = []\n",
    "with open(conllu_file_path, \"r\") as file:\n",
    "    sentences = [sentence for sentence in parse_conllu_incr(file)]\n",
    "\n",
    "sentence = sentences[0]\n",
    "print(f\"sentence.words: {sentence.words}\")\n",
    "print(f\"sentence.lemmas: {sentence.lemmas}\")\n",
    "print(f\"sentence.upos: {sentence.upos}\")\n",
    "print(f\"sentence.xpos: {sentence.xpos}\")\n",
    "print(f\"sentence.feats: {sentence.feats}\")\n",
    "print(f\"sentence.heads: {sentence.heads}\")\n",
    "print(f\"sentence.deprels: {sentence.deprels}\")\n",
    "print(f\"sentence.deps: {sentence.deps}\")\n",
    "print(f\"sentence.deps: {sentence.deps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7231be8-8ff4-4886-8432-d62e0537ca99",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CobaldDataset.__init__() missing 1 required positional argument: 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(sample) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupos\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupos_tagset(upos) \u001b[38;5;28;01mfor\u001b[39;00m upos \u001b[38;5;129;01min\u001b[39;00m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupos\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     40\u001b[0m         }\n\u001b[0;32m---> 44\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCobaldDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconllu_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m vocab \u001b[38;5;241m=\u001b[39m Vocabulary(dataset)\n\u001b[1;32m     47\u001b[0m vocab\u001b[38;5;241m.\u001b[39mupos_labels\n",
      "\u001b[0;31mTypeError\u001b[0m: CobaldDataset.__init__() missing 1 required positional argument: 'transform'"
     ]
    }
   ],
   "source": [
    "# DATA PIPELINE: YOUR CODE GOES HERE\n",
    "\n",
    "class CobaldDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, conllu_file, transform):\n",
    "        self.sentences = []\n",
    "        with open(conllu_file_path, \"r\") as file:\n",
    "            for sentence in parse_conllu_incr(file):\n",
    "                self.sentences.append(sentence)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = {\n",
    "            \"tokens\": self.sentences[index].words,\n",
    "            \"lemmas\": self.sentences[index].lemmas,\n",
    "            \"upos\": self.sentences[index].upos,\n",
    "            \"xpos\": self.sentences[index].xpos,\n",
    "            \"feats\": [str(feat) for feat in self.sentences[index].feats],\n",
    "            \"heads\": self.sentences[index].heads,\n",
    "            \"deprels\": self.sentences[index].deprels,\n",
    "            \"deps\": self.sentences[index].deps,\n",
    "        }\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)    \n",
    "        return sample\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, samples: list):\n",
    "        self.upos_labels = sorted(set([upos for sample in samples for upos in sample[\"upos\"]]))\n",
    "        self.upos_tagset = {upos: i for i, upos in enumerate(self.upos_labels)}\n",
    "        self.xpos_labels = sorted(set([upos for sample in samples for upos in sample[\"xpos\"]]))\n",
    "        self.xpos_tagset = {upos: i for i, upos in enumerate(self.xpos_labels)}\n",
    "        self.feats_labels = sorted(set([upos for sample in samples for upos in sample[\"feats\"]]))\n",
    "        self.feats_tagset = {upos: i for i, upos in enumerate(self.feats_labels)}\n",
    "        self.deps_tagset = {dep.keys() for sample in samples for dep in sample[\"deps\"]}\n",
    "\n",
    "    def encode(sample) -> dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            \"upos\": torch.Tensor([self.upos_tagset(upos) for upos in sample['upos']])\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "dataset = CobaldDataset(conllu_file_path)\n",
    "\n",
    "vocab = Vocabulary(dataset)\n",
    "vocab.upos_labels\n",
    "vocab.upos_tagset\n",
    "vocab.feats_tagset\n",
    "vocab.deps_tagset\n",
    "\n",
    "token_deps = {6: 'discourse', 7: 'aux'}\n",
    "[0,0,0,0,0,5,0,0,0]\n",
    "\n",
    "token_deps = {6: 'aux'}\n",
    "[0,0,0,0,0,1,0,0,0]\n",
    "\n",
    "\n",
    "[0,0,0,0,0,5,32,0,0]\n",
    "[0,0,0,0,0,1,0,0,0]\n",
    "\n",
    "dataset = CobaldDataset(conllu_file_path, transform=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb1540-35bd-4160-953c-e11d53a03459",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1deb2577-3ce8-4315-9372-5e1af881a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00218ff-0149-4473-8c11-772049fafcbd",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aefd37-6f82-413d-9223-4491c15a8e67",
   "metadata": {},
   "source": [
    "## Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb6f38-00fd-498d-a1e4-4367e997f396",
   "metadata": {},
   "source": [
    "<img src=\"img/NaiveTagger.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "1. Предобученный BERT строит эмбеддинги токенов предложения.\n",
    "2. Эмбеддинги подаются в головы-классификаторы.\n",
    "3. Каждая голова предсказывает свой тег для каждого эмбеддинга. В итоге для каждого эмбеддинга получится набор тегов - это и есть multi-task sequence tagging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8331b314-0434-44d8-b4b8-f9938b5808fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import PretrainedTransformerMismatchedEncoder\n",
    "from mlp_classifier import MLPClassifier\n",
    "\n",
    "# TAGGER: YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0141301-4692-4382-b868-b642b4117fb0",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294b5ca-536a-4445-b08a-6580671f4bba",
   "metadata": {},
   "source": [
    "<img src=\"img/Encoder.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e658034-8b8e-4f81-8484-93f8a427f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "131b2f01-67bb-4447-90a0-335bee42e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 8, 0],\n",
      "        [0, 1, 2, 3, 3, 4, 4, 5, 6, 0, 0, 0, 0, 0]])\n",
      "['<s>', '▁You', '▁are', '▁ki', 'dding', '▁', ',', '▁right', '▁?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m subtokens_embeddings \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#subtokens_embeddings[words_ids]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#words_ids.max(dim=0).values != -1\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m [t\u001b[38;5;241m:=\u001b[39mtorch\u001b[38;5;241m.\u001b[39munique_consecutive(word_ids) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mt\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word_ids \u001b[38;5;129;01min\u001b[39;00m words_ids]\n",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "    ['Well', 'does', \"n't\", 'that', '#NULL', 'include', 'rock', '?'],\n",
    "    ['You', 'are', 'kidding', ',', 'right', '?']\n",
    "]\n",
    "\n",
    "subtokens = tokenizer(\n",
    "    words,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    is_split_into_words=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "words_ids = torch.stack([\n",
    "    torch.tensor([word_id + 1 if word_id is not None else 0 for word_id in subtokens.word_ids(batch_idx)])\n",
    "    for batch_idx in range(len(tokens))\n",
    "])\n",
    "print(words_ids)\n",
    "#words_ids = torch.stack(words_ids)\n",
    "#print(words_ids)\n",
    "#words_ids += 1\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(subtokens.input_ids[1].tolist(), skip_special_tokens=False))\n",
    "model_output = model(**subtokens)\n",
    "subtokens_embeddings = model_output.last_hidden_state\n",
    "\n",
    "#subtokens_embeddings[words_ids]\n",
    "#words_ids.max(dim=0).values != -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8a9087d4-67f4-43d0-b55a-88bce48ddc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 1, 1, 0, 1],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  2,  1, -1, -1],\n",
       "        [ 1,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.zeros((2, 8), dtype=torch.int)\n",
    "mask[0, 4] = 1\n",
    "mask[0, 5] = 1\n",
    "mask[0, 7] = 1\n",
    "mask[1, 1] = 1\n",
    "print(mask)\n",
    "sentences_masks = mask.bool()\n",
    "\n",
    "counting_masks: list[LongTensor] = []\n",
    "for sentence_mask in sentences_masks:\n",
    "    nonnull_tokens_idxs = [i for i, is_null in enumerate(sentence_mask) if not is_null]\n",
    "    nonnull_tokens_idxs.append(len(sentence_mask))\n",
    "    nonnull_tokens_idxs = torch.LongTensor(nonnull_tokens_idxs)\n",
    "    counting_mask = torch.diff(nonnull_tokens_idxs) - 1\n",
    "    counting_masks.append(counting_mask)\n",
    "\n",
    "counting_masks_batched = torch.nn.utils.rnn.pad_sequence(\n",
    "    counting_masks,\n",
    "    batch_first=True,\n",
    "    # Use -1 to make sure it is never attended to.\n",
    "    # (if it is, the CUDA will terminate with an error that classes must be positive).\n",
    "    padding_value=-1\n",
    ")\n",
    "counting_masks_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fee855ad-0acf-4316-ac04-99805f569136",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m words_ids_expanded \u001b[38;5;241m=\u001b[39m words_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, n_subtokens, embedding_dim)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Use scatter_add_ to sum embeddings of tokens corresponding to the same word.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mwords_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_reduce_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwords_ids_expanded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubtokens_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_self\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#assert torch.allclose(words_embeddings[0, 3], torch.mean(subtokens_embeddings[0, 3:6], dim=0))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(words_embeddings[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], subtokens_embeddings[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index -1 is out of bounds for dimension 1 with size 9"
     ]
    }
   ],
   "source": [
    "valid_mask = (words_ids != 0)\n",
    "\n",
    "batch_size = len(words_ids)\n",
    "n_subtokens = subtokens_embeddings.size(1)\n",
    "n_words = torch.max(words_ids) + 2 # +2 since there are also 0 and -1 word_id.\n",
    "embedding_dim = subtokens_embeddings.size(2)\n",
    "\n",
    "words_embeddings = torch.zeros(\n",
    "    size=(batch_size, n_words, embedding_dim),\n",
    "    dtype=subtokens_embeddings.dtype,\n",
    "    #device=device\n",
    ")\n",
    "words_ids_expanded = words_ids.unsqueeze(-1).expand(batch_size, n_subtokens, embedding_dim)\n",
    "\n",
    "# Use scatter_add_ to sum embeddings of tokens corresponding to the same word.\n",
    "# self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n",
    "words_embeddings.scatter_reduce_(\n",
    "    dim=1,\n",
    "    index=words_ids_expanded,\n",
    "    src=subtokens_embeddings,\n",
    "    reduce='mean',\n",
    "    include_self=False\n",
    ")\n",
    "\n",
    "#assert torch.allclose(words_embeddings[0, 3], torch.mean(subtokens_embeddings[0, 3:6], dim=0))\n",
    "assert torch.allclose(words_embeddings[1, 2], subtokens_embeddings[1, 2])\n",
    "\n",
    "words_embeddings\n",
    "\n",
    "# # Divide by the count of tokens for each word to get the average.\n",
    "# token_counts = torch.bincount(batch.word_ids)\n",
    "# assert len(word_embeddings) == len(token_counts)\n",
    "# word_embeddings = word_embeddings / token_counts.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f0818f03-3b4d-4edf-ae5a-cb439959dd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.5958e-02,  2.0578e-02,  5.2515e-02,  3.4538e-02,  4.9695e-02,\n",
       "         2.4375e-02,  4.6327e-02, -4.6100e-02, -4.2507e-02, -1.4495e-01,\n",
       "         1.1918e-01, -1.6425e-02,  1.2501e-01,  1.4141e-01,  4.7271e-03,\n",
       "        -7.0020e-02, -3.1715e-03,  5.8356e-02,  4.1912e-02,  7.4019e-02,\n",
       "        -3.6298e-02, -1.8344e-02,  1.3122e-01,  7.0672e-03, -1.3485e-02,\n",
       "         2.5353e-02, -1.0478e-01, -4.2346e-02, -1.1621e-01,  2.4701e-02,\n",
       "         2.9747e-02,  1.1170e-02,  8.8466e-02, -1.8794e-03, -5.8947e-02,\n",
       "         5.8702e-02, -3.4253e-02,  4.1039e-02,  5.2156e-02,  5.7071e-02,\n",
       "         9.0317e-04, -3.3349e-02, -5.1162e-02,  1.1496e-01,  2.6062e-02,\n",
       "         7.7841e-03,  2.8885e-02,  7.6071e-02,  7.3318e-02, -5.7700e-02,\n",
       "         3.2926e-02,  1.8309e-01,  5.2991e-02,  3.6890e-02,  1.5286e-01,\n",
       "         1.6948e-01, -2.1963e-02,  1.4223e-01, -1.5735e-02,  2.7561e-01,\n",
       "        -5.1874e-04,  1.0980e-02,  1.1310e-02,  2.6556e-01,  5.1218e-02,\n",
       "        -2.3557e-02,  7.2160e-02, -7.7458e-03,  8.4758e-02,  3.8883e-02,\n",
       "         6.1244e-03,  1.1771e-01,  1.2496e-01, -7.7767e-02,  3.7081e-02,\n",
       "        -1.1460e-03,  8.6643e-02,  4.3225e-02,  1.3294e-02, -8.8602e-02,\n",
       "         7.3479e-03,  1.8871e-01, -4.8455e-03,  1.1158e-01,  2.4565e-01,\n",
       "         1.8830e-01, -9.7868e-05, -3.3144e-02, -2.9731e-02,  1.6497e-01,\n",
       "         1.6336e-02, -2.4038e-02,  1.5584e-01,  1.1848e-02, -3.0126e-02,\n",
       "        -6.3640e-02, -8.7484e-03,  7.5007e-03, -4.6101e-02,  2.0753e-01,\n",
       "        -4.0628e-02,  1.6167e-02,  4.4063e-02,  1.2578e-01,  4.5008e-02,\n",
       "        -6.0762e-04, -5.7242e-02,  3.4314e-02,  2.9607e-03,  1.0300e-01,\n",
       "         3.0002e-04, -1.9044e-02,  1.6460e-01,  7.7723e-02,  6.8326e-02,\n",
       "        -2.2336e-02, -7.1450e-02,  2.2621e-01,  3.7204e-02, -2.3917e-02,\n",
       "         6.9171e-02,  1.2826e-02,  3.5038e-02, -1.5394e-02, -5.1079e-02,\n",
       "        -7.8896e-02, -1.9919e-02, -3.6135e-02,  1.4330e-02,  7.1213e-02,\n",
       "        -3.0349e-02,  1.1506e-01,  5.5331e-03, -1.3833e-02, -3.1034e-02,\n",
       "        -3.8907e-02, -8.2905e-02, -1.6208e-02,  5.8218e-02,  9.1043e-02,\n",
       "         2.0089e-01,  1.3204e-01, -1.1042e-01,  1.7464e-01, -2.7800e-02,\n",
       "        -1.4269e-01,  2.5665e-02,  4.1324e-02,  1.9343e-02, -1.4213e-02,\n",
       "        -1.5366e-02, -1.2936e-02, -1.4898e+00,  4.4277e-04, -1.6309e-02,\n",
       "         1.2132e-01, -5.3870e-02,  2.0947e-02, -1.0671e-01, -2.9242e-02,\n",
       "        -1.1635e-01, -2.9773e-03, -1.6471e-02,  1.1364e-01, -7.8792e-02,\n",
       "        -2.1758e-01, -5.3529e-03,  6.5080e-02, -7.2628e-03, -8.4197e-02,\n",
       "         5.3887e-02,  6.8741e-03,  7.2791e-02,  6.7117e-02,  3.4231e-02,\n",
       "         1.7869e-02, -1.1120e-02,  1.0523e-01,  5.2247e-02, -3.2365e-02,\n",
       "        -9.5762e-02,  2.1190e-02,  8.8344e-02,  3.8795e-02, -3.6184e-01,\n",
       "         2.9183e-02, -2.7865e-02,  1.0507e-01,  7.1041e-02, -6.5942e-02,\n",
       "        -3.0608e-02, -7.0301e-02,  6.6489e-03,  1.5787e-02,  4.6318e-02,\n",
       "        -6.9479e-02, -8.4180e-02,  4.5491e-03, -1.4674e-02, -4.6531e-02,\n",
       "         1.6388e-02,  5.7845e-04,  2.1291e-01,  1.3344e-01,  1.2513e-02,\n",
       "         1.0860e-01, -8.8976e-02, -6.4677e-02,  1.2202e-02, -5.4787e-02,\n",
       "        -3.5025e-02, -5.4536e-02, -6.8749e-02, -3.5682e-02,  7.1493e-02,\n",
       "        -4.2002e-02, -4.7772e-02, -1.1299e-01,  8.0612e-03,  5.4236e-02,\n",
       "        -3.3247e-02, -6.2339e-01, -2.3690e-01, -4.5131e-02,  1.6248e-02,\n",
       "         5.6421e-02,  4.6931e-02, -2.1636e-02,  1.1072e-01, -3.2387e-02,\n",
       "        -4.3288e-02,  1.5128e-01, -4.3472e-02, -1.0583e-01,  7.5740e-03,\n",
       "         1.9640e-02,  1.1299e-01, -4.5730e-02,  1.3992e-02, -9.3972e-01,\n",
       "        -6.7097e-02, -3.0095e-03,  2.9812e-02,  1.1570e-01, -1.9746e-01,\n",
       "         6.3896e-02, -1.0699e-01,  6.6403e-02, -1.8349e-01,  2.4373e-01,\n",
       "         7.3972e-02,  4.8105e-02, -5.0595e-02,  3.0557e-02,  5.1906e-02,\n",
       "         1.4106e-01,  9.3824e-02, -2.3426e-01,  6.2913e-02,  1.1702e+00,\n",
       "        -9.8662e-02,  8.9134e-02,  3.2225e-02, -2.8608e-01,  4.8701e-02,\n",
       "         1.0682e-01,  1.0458e-01,  6.7006e-02,  3.3312e-02, -1.4954e-02,\n",
       "        -1.4959e-02, -7.4532e-02,  7.2627e-02,  1.3804e-01,  5.6208e-02,\n",
       "         3.4222e-02, -2.2230e-01, -1.1609e-01, -2.6076e-01,  8.4743e-02,\n",
       "         7.4514e-02,  2.9200e-01,  5.6857e-02,  8.1667e-02, -2.8142e-02,\n",
       "         5.1626e-02, -6.0934e-03,  1.3518e-01,  2.8982e-01, -8.4658e-02,\n",
       "        -4.8448e-02,  2.1400e-01,  3.5402e-02,  2.4225e-01,  2.7388e-01,\n",
       "        -1.4205e-02, -1.2772e-01, -3.3012e-02, -2.0243e-02, -1.2847e-01,\n",
       "        -4.4803e-03,  3.9075e-02, -3.3161e-01, -1.9132e-02,  2.1091e-01,\n",
       "         2.3050e-02,  7.6302e-01,  9.3179e-02, -4.7472e-02,  1.3510e-01,\n",
       "        -4.0925e-02,  7.5956e-03, -1.0707e-01, -9.2199e-02,  2.6223e-01,\n",
       "        -4.2898e-02,  6.0041e-02,  9.7028e-03,  4.4566e-02, -5.2611e-02,\n",
       "         1.9535e-03, -2.1151e-02,  5.5428e-02, -8.6215e-02, -1.7828e-02,\n",
       "        -1.7170e-01, -1.9911e-02,  9.1296e-02,  1.9328e-01,  2.8607e-02,\n",
       "        -6.3355e-02,  2.7710e-02, -1.2585e-01, -4.5997e-03, -2.1519e-01,\n",
       "         1.2042e-01,  1.0184e-01,  6.8023e-03, -4.5705e-03, -7.0863e-02,\n",
       "        -9.1754e-02,  1.0497e-01, -2.5905e-01, -9.8098e-02,  1.3643e-01,\n",
       "         1.4358e-01, -1.4991e-02, -1.4214e-01, -7.1209e-03,  1.1598e-01,\n",
       "         1.0506e-02, -1.6683e-01,  2.9013e-03,  2.6920e-01, -5.5297e-03,\n",
       "        -4.4650e-02,  2.3078e-01,  2.6488e-01,  6.0291e-02, -3.6628e-02,\n",
       "        -7.5395e-02, -2.1287e-02, -3.5470e-02,  1.7469e-02,  2.7864e-02,\n",
       "         4.1854e-02, -8.8408e-02, -2.0654e-03, -1.5440e-02,  2.3720e-02,\n",
       "         1.7253e-01,  1.5937e-02, -2.4447e-02,  1.2651e-02, -1.2243e-02,\n",
       "         2.8275e-02, -9.7082e-02,  2.0773e-01, -3.8949e-02,  2.2866e-02,\n",
       "         2.7408e-02,  1.3172e-01, -1.9060e-02, -4.6525e-02,  2.8828e-02,\n",
       "         2.5611e-01,  7.8786e-02, -7.0381e-02, -5.7248e-02,  2.2214e-03,\n",
       "        -9.4296e-02, -1.4122e-01, -3.2078e-02, -1.9817e-02,  5.4793e-02,\n",
       "        -1.7159e-01,  6.6049e-02, -1.7760e-01,  1.0956e-01,  3.9260e-02,\n",
       "        -1.1641e-01, -1.4292e-02, -2.8082e-02, -5.9600e-02, -4.8167e-02,\n",
       "         9.4371e-02,  1.2521e-02,  3.1730e-02,  3.4598e-02,  7.6988e-03,\n",
       "         6.5453e-02, -2.0817e-01, -1.2238e-02, -4.4607e-02,  1.3445e-01,\n",
       "         1.4416e-02,  4.6656e-02,  2.0415e-02, -1.6505e-01, -1.8777e-03,\n",
       "         9.3585e-02,  1.4768e-03,  1.3479e-02,  4.4310e-02,  6.5546e-02,\n",
       "        -8.3363e-02, -2.2854e-02,  2.3716e-02, -5.1562e-02, -2.4669e-02,\n",
       "         8.4676e-02,  4.0105e-02,  7.7708e-02, -3.2722e-02, -1.2382e-02,\n",
       "        -6.8273e-02,  6.4402e-02, -5.1632e-02, -1.5474e-02,  3.5651e-02,\n",
       "        -2.0704e-02,  8.7809e-02, -1.2389e-01, -5.8969e-02,  8.7713e-02,\n",
       "         2.2950e-02, -5.5200e-02,  1.7779e-02, -1.2465e-01, -6.4366e-03,\n",
       "         7.5625e-03,  3.6554e-02,  1.7354e-02,  3.7482e-02, -3.9363e-03,\n",
       "        -7.5800e-04,  5.9473e-02,  6.5437e-02,  2.9188e-03, -3.1959e-02,\n",
       "        -1.0400e-01, -3.9250e-02,  1.1180e-01,  5.0350e-02,  1.4337e-01,\n",
       "        -3.3631e-01, -1.8022e-02, -2.3559e-02, -8.3052e-02, -5.2585e-02,\n",
       "         2.9911e-02, -7.3565e-02,  7.5072e-02, -1.2198e-01, -2.9676e-02,\n",
       "         6.7590e-03,  1.3006e-01,  1.2224e-01,  9.5641e-02, -3.8663e-02,\n",
       "         2.5027e-02,  3.5455e-02, -1.0533e-02,  7.6214e-03, -2.5147e-02,\n",
       "        -7.7331e-02, -1.7056e-02,  4.9038e-02,  4.3710e-02, -5.3089e-03,\n",
       "         5.0004e-02,  3.5452e-02, -3.7830e-02, -1.9177e-03, -8.7886e-02,\n",
       "        -2.4313e-03, -1.5902e-01,  4.7934e-02,  2.0604e-01,  6.5211e-02,\n",
       "        -7.2776e-02, -6.6333e-02,  7.7350e-02, -5.4821e-03,  1.5375e-01,\n",
       "         3.9547e-02,  9.6844e-02, -1.3790e-01, -4.7633e-02,  3.1065e-02,\n",
       "         1.4946e-01,  7.5813e-02,  8.3833e-02, -1.3747e-02,  5.3464e-02,\n",
       "         1.2960e-01, -4.9178e-02,  2.7020e-02, -6.2575e-02,  2.7474e-03,\n",
       "        -1.1083e-01,  4.2215e-02, -5.2586e-02,  7.0866e-02,  4.9129e-02,\n",
       "        -2.3138e-02,  7.3847e-02,  1.0344e-01, -4.6849e-03,  1.4074e-01,\n",
       "        -8.8975e-02,  2.0290e-03, -2.2242e-02, -3.5268e-01, -2.4448e-02,\n",
       "        -1.6079e-01, -3.0448e-02, -2.3514e-02,  1.2674e-01,  4.9268e-02,\n",
       "        -2.0143e-01,  5.3414e-03, -3.9379e-02, -5.6913e-02,  4.9578e-02,\n",
       "        -2.8169e-02, -9.0304e-02,  2.3874e-02, -2.5653e-02,  4.2058e-02,\n",
       "         4.3184e-02,  8.7192e-03,  3.5089e-02, -4.5385e-02, -1.3736e-01,\n",
       "         2.1747e-02,  1.5835e-01,  1.8860e-02,  1.0100e-01, -5.2680e-03,\n",
       "        -1.4143e-01, -3.0405e-02,  7.5705e-02, -9.2017e-05, -1.2045e-01,\n",
       "         6.0604e-02, -1.5330e-02, -3.9147e-02,  1.4990e-02,  5.8186e-03,\n",
       "         1.9680e-01,  3.7164e-02, -3.2634e-02, -1.0392e-01, -9.0025e-02,\n",
       "        -8.5626e-02,  2.5532e-02,  3.4398e-02, -4.6249e-02, -1.6792e-01,\n",
       "         7.7505e-03,  1.1745e-01, -1.2129e-01,  1.6841e-02,  7.8109e-02,\n",
       "        -3.9324e-02,  1.4248e-01,  2.5787e-01, -4.9512e+00, -2.6853e-02,\n",
       "         6.4447e-02,  3.8847e-02, -1.1208e-02,  9.1793e-02,  2.2392e-02,\n",
       "         6.1386e-03, -2.3578e-02, -6.6130e-02,  3.8627e-02,  8.6043e-02,\n",
       "         1.1941e-01,  1.6198e-02,  9.3174e-02,  1.9299e-01, -4.0562e-02,\n",
       "         4.6274e-02, -6.3097e-02, -2.4778e-02, -3.3490e-02, -2.6975e-02,\n",
       "        -4.0950e-03,  1.6837e-01, -2.5714e-02,  3.0657e-02,  2.0852e-02,\n",
       "        -6.4472e-02, -3.7675e-02, -4.5478e-02,  1.5328e-02,  2.2909e-01,\n",
       "        -1.5911e-02, -3.6179e-02, -2.3965e-03,  2.8547e-02,  2.4230e-03,\n",
       "         1.8004e-02,  1.1544e-01, -1.9162e-01,  1.3467e-02,  1.3671e-01,\n",
       "         1.4272e-01, -6.6993e-03, -6.3165e-02,  9.9414e-02,  6.5066e-02,\n",
       "        -4.7649e-02, -7.6450e-02, -3.3235e-02,  1.7856e-01, -2.6392e-02,\n",
       "        -1.3857e-01,  3.8924e-02, -6.2994e-02,  2.3787e-03,  5.1006e-02,\n",
       "        -8.6913e-02,  4.5880e-02, -8.1790e-02,  4.0561e-02,  3.9285e-02,\n",
       "        -1.1284e-01, -4.4913e-02,  6.6941e-02,  2.8970e-02,  6.2011e-03,\n",
       "         5.5326e-02,  1.8636e-02,  1.9019e-02, -5.9025e-03,  2.2472e-02,\n",
       "        -1.0796e-01,  1.2814e-01, -1.6978e-02,  1.0928e-01,  9.4928e-02,\n",
       "        -7.9761e-02,  3.5974e-02,  6.6521e-02, -8.1497e-02, -1.3206e-01,\n",
       "        -5.0964e-02, -5.4113e-02, -4.2409e-02, -1.2998e-01,  7.5633e-02,\n",
       "         3.4498e-02,  5.7847e-02, -3.7411e-02,  2.0405e-02,  9.4703e-02,\n",
       "        -6.3265e-02, -8.5243e-03, -3.8264e-03,  7.9592e-02, -2.2875e-01,\n",
       "         9.0841e-03,  8.9925e-03, -5.2513e-02,  1.1682e-02,  2.5264e-01,\n",
       "         5.6228e-02, -1.2756e-01,  8.3766e-02,  1.4840e-01,  7.0894e-03,\n",
       "         5.5489e-02, -1.4812e-03,  5.1644e-02,  2.3167e-01, -5.9904e-02,\n",
       "         1.2626e-02,  5.3770e-02, -3.6925e-02, -7.3172e-03, -4.0632e-02,\n",
       "        -8.7276e-02,  2.3834e-02, -2.4670e-02,  5.2225e-02,  6.1368e-02,\n",
       "         1.6685e-02, -3.2920e-03, -3.8178e-02, -3.9260e-02,  4.2133e-02,\n",
       "        -1.7311e-01, -2.5403e-02,  1.6379e-03,  5.6158e-02, -9.7122e-02,\n",
       "        -9.5025e-02,  4.3406e-02,  6.9184e-02, -3.7051e-01, -3.5638e-02,\n",
       "         3.3789e-02,  2.3739e-02,  3.6171e-02,  1.5162e-01, -6.3127e-02,\n",
       "         6.9437e-02, -1.0790e-01,  1.7633e-01, -1.0907e-01,  1.0098e-01,\n",
       "        -5.4917e-02,  3.5466e-01,  5.2884e-02,  8.1893e-02, -1.6093e-01,\n",
       "        -8.1478e-03,  1.8008e+01,  1.2451e-02, -2.9701e-02,  1.2365e-03,\n",
       "         4.8389e-02,  4.5885e-02, -4.4292e-03, -8.1481e-02,  3.0407e-01,\n",
       "        -5.9469e-02,  6.3393e-02,  1.8335e-02, -9.6799e-02, -7.5153e-02,\n",
       "         2.1160e-02,  2.1990e-02,  2.9010e-02, -2.0427e-02,  1.0295e-01,\n",
       "         2.7634e-02,  9.1375e-02, -2.4063e-02, -5.2556e-02,  1.4245e-02,\n",
       "         2.4578e-01,  5.3536e-02,  3.1787e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_embeddings_red = torch.zeros(\n",
    "    size=(batch_size, n_words, embedding_dim),\n",
    "    dtype=subtokens_embeddings.dtype,\n",
    "    #device=device\n",
    ")\n",
    "\n",
    "# Use scatter_add_ to sum embeddings of tokens corresponding to the same word.\n",
    "# self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n",
    "\n",
    "words_embeddings_red[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4681f46-4d37-4af6-9d08-3be3dccf8a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m     token_embeddings \u001b[38;5;241m=\u001b[39m token_sums \u001b[38;5;241m/\u001b[39m (token_counts \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)  \u001b[38;5;66;03m# Avoid division by zero\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token_embeddings\n\u001b[0;32m---> 36\u001b[0m \u001b[43maverage_subtokens_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubtokens_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m, in \u001b[0;36maverage_subtokens_to_tokens\u001b[0;34m(subtokens_embeddings, word_ids)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage_subtokens_to_tokens\u001b[39m(subtokens_embeddings, word_ids):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Convert word_ids (list of lists) into a padded tensor\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_ids)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Create a padded tensor with -1 for None values\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage_subtokens_to_tokens\u001b[39m(subtokens_embeddings, word_ids):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Convert word_ids (list of lists) into a padded tensor\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m word_ids)\n\u001b[1;32m      4\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_ids)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Create a padded tensor with -1 for None values\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "def average_subtokens_to_tokens(subtokens_embeddings, words_ids):\n",
    "    # Convert word_ids (list of lists) into a padded tensor\n",
    "    max_len = max(len(seq) for seq in word_ids)\n",
    "    batch_size = len(word_ids)\n",
    "\n",
    "    # Create a padded tensor with -1 for None values\n",
    "    padded_word_ids = torch.full((batch_size, max_len), -1, dtype=torch.long)\n",
    "    for i, seq in enumerate(word_ids):\n",
    "        padded_word_ids[i, :len(seq)] = torch.tensor([w if w is not None else -1 for w in seq])\n",
    "\n",
    "    # Mask for valid subtokens (where word_ids is not None)\n",
    "    valid_mask = padded_word_ids != -1\n",
    "\n",
    "    # Determine the max number of tokens\n",
    "    max_tokens = padded_word_ids.max().item() + 1\n",
    "\n",
    "    # One-hot encode the word_ids for token aggregation\n",
    "    one_hot_ids = torch.zeros(batch_size, max_tokens, max_len, device=subtokens_embeddings.device)\n",
    "    one_hot_ids.scatter_(1, padded_word_ids.unsqueeze(1).clamp(min=0), 1.0)  # Clamp to handle -1 indices\n",
    "\n",
    "    # Apply the mask to ignore invalid positions\n",
    "    one_hot_ids = one_hot_ids * valid_mask.unsqueeze(1).float()\n",
    "\n",
    "    # Sum embeddings for each token\n",
    "    token_sums = torch.bmm(one_hot_ids, subtokens_embeddings)\n",
    "\n",
    "    # Count the number of subtokens for each token\n",
    "    token_counts = one_hot_ids.sum(dim=2).unsqueeze(-1)\n",
    "\n",
    "    # Compute the average\n",
    "    token_embeddings = token_sums / (token_counts + 1e-8)  # Avoid division by zero\n",
    "\n",
    "    return token_embeddings\n",
    "\n",
    "\n",
    "average_subtokens_to_tokens(subtokens_embeddings, subtokens.word_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f670b-e31f-4832-885a-d2e0ab7bdf4e",
   "metadata": {},
   "source": [
    "### Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8d1e4-385a-4af6-b2a4-f16fc030f277",
   "metadata": {},
   "source": [
    "All the heads (except for syntactic ones) are multilayer perceptrons with ReLU activation. Yet, there are few tricks that worth talking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde7dec-de4c-49ea-a557-c3185152bc72",
   "metadata": {},
   "source": [
    "#### Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849d38e-78b5-47bc-9db9-fcfaca68842e",
   "metadata": {},
   "source": [
    "Lemmatization classifier predicts *lemmatization rules*. Lemmatization rule is a sequence of three modifications that have to be applied to a word to obtain its lemma:\n",
    "\n",
    "1. Cut $N$ symbols from the prefix of the word,\n",
    "2. Cut $N$ symbols from the suffix,\n",
    "3. Append a specific sequence of symbols to the suffix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99391713-8942-4a1d-8f4e-b14e573a629f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LemmaRule(cut_prefix=0, cut_suffix=1, append_suffix='унда')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lemmatize_helper import predict_lemma_rule\n",
    "\n",
    "\n",
    "\n",
    "lemma_rule = predict_lemma_rule(word=\"сек.\", lemma=\"секунда\")\n",
    "lemma_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2da312e7-b4bd-4039-8460-85443d6c1409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LemmaRule(cut_prefix=3, cut_suffix=0, append_suffix='')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_lemma_rule(word=\"препроцессинг\", lemma=\"процессинг\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dd388e1-8db6-4663-b62f-6ea1828e3b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cut_prefix=0|cut_suffix=3|append_suffix=ouse'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(lemma_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef2743-7c4a-42a5-8b98-43f6776b841f",
   "metadata": {},
   "source": [
    "#### POS-&-Feats classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c64df6-8ecf-4423-898d-ea85e280b9bd",
   "metadata": {},
   "source": [
    "Part-of-speech and grammatical features can be predicted as two independent tags, but this naive strategy leaves room for compatibility errors. For example, a separate POS head may tag a token as a noun, whilst a standalone Feats classifier may predict a present tense for the token, which is incorrect tagging, because nouns don't have time category.\n",
    "\n",
    "To avoid such behavior, we use a single POS-&-Feats classifier that predicts joint part-of-speech and grammatical features tags. Note, that since POS and grammatical features are heavily correlated, the number of joint classes is much smaller than the Cartesian product of two tags, so the classification approach remains feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e972c4-bdfb-4c78-806a-975fe8ea3fed",
   "metadata": {},
   "source": [
    "<img src=\"img/PosFeatsClassifier.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96ea77-aa79-4b66-9ebc-d4ea2ee6e5cb",
   "metadata": {},
   "source": [
    "#### Basic syntax analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a09b6-3d8f-4ba0-a7eb-4a90ba8d4a1b",
   "metadata": {},
   "source": [
    "Syntax analyzer is a [biaffine dependency classifier](https://arxiv.org/abs/1611.01734) that scores embeddings relations.\n",
    "\n",
    "<img src=\"img/BiaffineClassifier.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "The scoring function for a head-dependent pair $(h^{(head)}, h^{(dep)})$ is:\n",
    "\n",
    "$S(h^{(head)}, h^{(dep)}) = h^{(head) \\top} U h^{(dep)}$\n",
    "\n",
    "$S(h^{(head)}, h^{(dep)}) = h^{(head) \\top} U h^{(dep)} + U_{row}^\\top h^{(dep)} + U_{col} h^{(head)}+ b$\n",
    "л\n",
    "Where:\n",
    "\n",
    "- $h$ and $d$: The head and dependent representations (from their respective MLPs).\n",
    "- $U$: A bilinear weight matrix capturing pairwise interactions.\n",
    "- $U_{row}$ and $U_{col}$: Additional row and column extending $U$, that account for a weight vectors for individual contributions of the head and dependent.\n",
    "\n",
    "Hint: one can represent a linear layer with bias: $Wx + b$ as a linear layer without bias: $W'x'$, where $W' = concat(W, b)$ and $x' = concat(x, 1)$.\n",
    "\n",
    "This allows the model to simultaneously consider independent word-level features and interaction-based pairwise features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08eae9-f8b0-49b9-b6d3-287db0222c1b",
   "metadata": {},
   "source": [
    "The classifier scores directed arcs between pairs of tokens using the bilinear transformation, and then applies a softmax to a scoring matrix \\\\( S^{(arc)} \\\\) to model a probability distribution \\\\(P\\\\) over the arcs of each token:\n",
    "\n",
    "$ P = softmax(S^{(arc)}). $\n",
    "\n",
    "To build the arcs \\\\( \\mathcal{A} \\\\) of a dependency tree, we either greedily choose the most probable arc for each token while training or use Chu-Liu-Edmonds algorithm to find a spanning arborescence of a maximum probability at the inference:\n",
    "\n",
    "$\n",
    "\\mathcal{A} = \n",
    "\\begin{cases} \n",
    "argmax(P), & \\text{if training,} \\\\ \n",
    "ChuLiuEdmonds(P), & \\text{if testing.}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "Finally, we select the relations \\\\( \\mathcal{L} \\\\)  towards the predicted arcs:\n",
    "\n",
    "$ \\mathcal{L} = argmax(S^{(rel)} [\\mathcal{I}(\\mathcal{A})]), $\n",
    "\n",
    "where \\\\( S^{(rel)} \\\\) is a tensor of relations scores and \\\\( \\mathcal{I}(\\mathcal{A}) \\\\) is indices of arcs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ad791-4b9e-4ad0-9455-75e00adf62c4",
   "metadata": {},
   "source": [
    "#### Enhanced syntax analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd48147-398d-4cd1-8d7a-3b39e516fe20",
   "metadata": {},
   "source": [
    "In contrast to basic syntax, the enhanced dependency graph is not restricted to a DAG and can represent an arbitrary graph. To account for this, we use dependency graph parser.\n",
    "\n",
    "The graph parser utilizes the same bilinear transformation the tree parser does, but models each arc probability independently using a multi-label approach with a binary cross-entropy loss (sigmoid) instead of multi-class cross-entropy loss (softmax) and selects all the confident arcs:\n",
    "\n",
    "$ P = sigmoid(S^{(arc)}), $\n",
    "\n",
    "$ \\mathcal{A} = [P > 0.5]. $\n",
    "\n",
    "This distributional relaxation allows the enhanced analyzer to predict multiple edges per token, accounting for the non-acyclic nature of enhanced syntactic graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422e899-5828-4b65-a91c-c200f16c0773",
   "metadata": {},
   "source": [
    "## Null predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e202d4c-85f4-446c-bf52-b49fff7e196d",
   "metadata": {},
   "source": [
    "Training sentence contains null tokens, while the test one has nulls removed. The objective is to restore the original nulls given a test sentence.\n",
    "\n",
    "We introduce a concept of *counting mask* that shows how many nulls follow a non-null token in the training sentence. The mask has a length of a test sentence, and value \\\\( N \\\\) at the position \\\\( i \\\\) means that the \\\\(i\\\\)-th non-null token is followed by \\\\( N \\\\) consecutive nulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dcc3bdab-71f8-4bba-8ab0-1e8cb8e4b33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_counting_mask(words: list[str]):\n",
    "    nonnull_idxs = [i for i, word in enumerate(words) if word != '#NULL']\n",
    "    # Append sentence length as a finishing index.\n",
    "    nonnull_idxs.append(len(words))\n",
    "    nonnull_idxs = torch.LongTensor(nonnull_idxs)\n",
    "    counting_mask = torch.diff(nonnull_idxs) - 1\n",
    "    return counting_mask\n",
    " \n",
    "\n",
    "words = ['Quick', 'brown', '#NULL', '#NULL', 'fox', '#NULL']\n",
    "# words_without_nulls = ['Quick', 'brown', 'fox']\n",
    "\n",
    "# => 0 nulls after Quick, 2 nulls after brown and 1 null after fox\n",
    "\n",
    "build_counting_mask(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24ac31f-a32c-454b-8a27-e0725019597d",
   "metadata": {},
   "source": [
    "Note, that the counting mask ignores the leading nulls, as there is no token to follow, e.g.:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23b65fb9-0306-45f8-88a9-7476b1500cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_counting_mask(['#NULL', 'On', 'that', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49d591-d2d0-4a24-a5c3-73fd887c285c",
   "metadata": {},
   "source": [
    "To account for this issue, we prepend an auxiliary token to the beginning of a test sentence prior to mask construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc12c1-4d59-44cd-bbc9-1ccbb6f548c1",
   "metadata": {},
   "source": [
    "Given the counting mask, ellipsis restoration becomes as simple as sequence labeling. We add another head null classifier that predicts the number of nulls to be inserted after a token.\n",
    "\n",
    "It The pipeline gets split into two stages: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a78b2f-39b7-4e3d-a613-57eb81cc9136",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f7509-0a5c-4f80-a60a-22c49974b4f8",
   "metadata": {},
   "source": [
    "At the inference, null predictor retrieves the elided tokens and adds them to an input sentence, then the refined sentence is parsed as usual:\n",
    "\n",
    "<img src=\"img/ParserTrain.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "During the training, sentences with and without nulls are passed through the encoder independently, so the tagging heads are always trained upon the well-formed sentences, and not the restored ones:\n",
    "\n",
    "<img src=\"img/ParserTest.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "It is similar to a teacher forcing approach used in sequence generation, where a ground truth token is fed to a model instead of the previously predicted word to avoid error accumulation and speed up the training.\n",
    "\n",
    "We jointly optimize the null classifier with the tagging heads by adding up the losses. The encoder aggregates the gradients from all classifiers and updates the weights once per training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cb83034-a4f3-4a2a-a86c-ceebd702d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARSER: YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554ed28-8ef7-4b44-aa98-1d5a6bc46256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
