{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c43489-26b3-475a-89a6-76fa7aa9d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.token import Token\n",
    "from common.sentence import Sentence\n",
    "from common.parse_conllu import parse_conllu_incr\n",
    "\n",
    "\n",
    "def convert_sentence_to_dict(sentence: Sentence) -> dict:\n",
    "    return {\n",
    "        \"words\": sentence.words,\n",
    "        \"lemmas\": sentence.lemmas,\n",
    "        \"upos\": sentence.upos,\n",
    "        \"xpos\": sentence.xpos,\n",
    "        \"feats\": sentence.feats,\n",
    "        \"heads\": sentence.heads,\n",
    "        \"deprels\": sentence.deprels,\n",
    "        \"deps\": sentence.deps,\n",
    "        \"miscs\": sentence.miscs,\n",
    "        \"deepslots\": sentence.semslots,\n",
    "        \"semclasses\": sentence.semclasses,\n",
    "        \"metadata\": sentence.metadata\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_conllu_to_hf(file_path: str) -> Iterable[dict]:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for sentence in parse_conllu_incr(file):\n",
    "            yield convert_sentence_to_dict(sentence)\n",
    "\n",
    "\n",
    "def build_raw_dataset(file_paths: dict[str, str]) -> DatasetDict:\n",
    "    features = Features({\n",
    "        \"words\": Sequence(Value(\"string\")),\n",
    "        \"lemmas\": Sequence(Value(\"string\")),\n",
    "        \"upos\": Sequence(Value(\"string\")),\n",
    "        \"xpos\": Sequence(Value(\"string\")),\n",
    "        \"feats\": Sequence(Value(\"string\")),\n",
    "        \"heads\": Sequence(Value(\"int32\")),\n",
    "        \"deprels\": Sequence(Value(\"string\")),\n",
    "        \"deps\": Sequence(Value(\"string\")),\n",
    "        \"miscs\": Sequence(Value(\"string\")),\n",
    "        \"deepslots\": Sequence(Value(\"string\")),\n",
    "        \"semclasses\": Sequence(Value(\"string\")),\n",
    "        \"metadata\": Value(\"string\")\n",
    "    })\n",
    "\n",
    "    splits = {}\n",
    "    for split_name, file_path in file_paths.items():\n",
    "        splits[split_name] = Dataset.from_generator(convert_conllu_to_hf, gen_kwargs={\"file_path\": file_path}, features=features)\n",
    "    return DatasetDict(splits)\n",
    "\n",
    "\n",
    "def create_dataset():\n",
    "    file_paths = {\n",
    "        \"train\": \"../data/train.conllu\",\n",
    "        \"validation\": \"../data/validation.conllu\",\n",
    "        \"test\": \"../data/test_clean.conllu\",\n",
    "    }\n",
    "    dataset = build_raw_dataset(file_paths)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e948b7c-8fb0-4cfa-9fd0-49a37617f1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e2d0a67c4741708d7c77640cf77f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6912 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6175e5a6df45ebaf983506641353e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04296594bbdd49b9a8d0b5e7f5da8f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea500be700748feabc01b0f5fc6e84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/6912 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1e5c46adfd4341b4b93e089c4bb052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be91e755723e4444becb68fff55f0d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'words': ['The',\n",
       "  'firm',\n",
       "  \"'s\",\n",
       "  'snowmobile',\n",
       "  'division',\n",
       "  'and',\n",
       "  'defence',\n",
       "  'services',\n",
       "  'unit',\n",
       "  'were',\n",
       "  'also',\n",
       "  'sold',\n",
       "  'and',\n",
       "  'Bombardier',\n",
       "  'started',\n",
       "  'the',\n",
       "  'development',\n",
       "  'of',\n",
       "  'a',\n",
       "  'new',\n",
       "  'aircraft',\n",
       "  'seating',\n",
       "  '110',\n",
       "  'to',\n",
       "  '135',\n",
       "  'passengers',\n",
       "  '.'],\n",
       " 'lemma_rules': tensor([  0,   0,   0,   0,   0,   0,   0,  13,   0, 143,   0,  74,   0,   0,\n",
       "          32,   0,   0,   0,   0,   0,   0,  63,   0,   0,   0,  13,   0],\n",
       "        device='cuda:0'),\n",
       " 'joint_pos_feats': tensor([ 53,  79,  96,  79,  79,  50,  79,  78,  79,  34,  21, 192,  50, 152,\n",
       "         179,  53,  79,  12,  54,   2,  79, 195,  85,  12,  85,  78, 160],\n",
       "        device='cuda:0'),\n",
       " 'deps_ud': tensor([[ 0, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0, 34,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 38,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 48,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 36,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 16,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 41,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0, 11,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0, 20,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  7,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 32,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0, 42,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0, 11,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0, 32,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0, 41,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 47,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0'),\n",
       " 'deps_eud': tensor([[  0,  55,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,  42,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,  46,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 122,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,  43,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,  46,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,  46,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,  49,   0,   0,   0,   0,   0,   0, 122,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  41,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  37,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 200,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           43,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          120,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  49,   0,   0,\n",
       "          200,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,  55,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          126,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,  42,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,  55,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,  38,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,  93,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 127,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  42,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 110,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0, 126,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 198,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "        device='cuda:0'),\n",
       " 'miscs': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 1, 2], device='cuda:0'),\n",
       " 'deepslots': tensor([134, 132, 134,  72,  72, 134,  74, 119,  72, 134,   1,  95, 134,   5,\n",
       "          95, 134,  74, 134, 134, 129,  72,  85, 105, 134, 105,  72, 134],\n",
       "        device='cuda:0'),\n",
       " 'semclasses': tensor([ 18,  96, 541, 520, 249, 113, 126, 521, 249,  20, 140, 416, 113,  96,\n",
       "          23,  18, 435, 272,  18,  78, 520, 110,  78, 272,  78,  25, 541],\n",
       "        device='cuda:0'),\n",
       " 'metadata': '{\\'sent_id\\': \\'2049\\', \\'text\\': \"The firm\\'s snowmobile division and defence services unit were also sold and Bombardier started the development of a new aircraft seating 110 to 135 passengers.\"}'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, ClassLabel, Array2D\n",
    "\n",
    "import torch\n",
    "\n",
    "from lemmatize_helper import predict_lemma_rule\n",
    "\n",
    "\n",
    "def dict_from_str(s: str) -> dict:\n",
    "    \"\"\"One cannot simply convert a string representation of a dict to a dict.\"\"\"\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "\n",
    "def preprocess(batch: dict[str, list]) -> dict[str, list]:\n",
    "    \"\"\"Return lemma rules and joint pos-feats columns for a batch.\"\"\"\n",
    "    words = batch[\"words\"]\n",
    "    lemmas = batch[\"lemmas\"]\n",
    "    upos = batch[\"upos\"]\n",
    "    xpos = batch[\"xpos\"]\n",
    "    feats = batch[\"feats\"]\n",
    "    heads = batch[\"heads\"]\n",
    "    deprels = batch[\"deprels\"]\n",
    "    deps = batch[\"deps\"]\n",
    "\n",
    "    lemma_rules: list[str] = None\n",
    "    if lemmas is not None:\n",
    "        lemma_rules = [\n",
    "            str(predict_lemma_rule(word if word is not None else '', lemma if lemma is not None else ''))\n",
    "            for word, lemma in zip(words, lemmas, strict=True)\n",
    "        ]\n",
    "\n",
    "    joint_pos_feats: list[str] = None\n",
    "    if upos is not None and xpos is not None and feats is not None:\n",
    "        joint_feats = [\n",
    "            '|'.join([f\"{k}={v}\" for k, v in dict_from_str(feat).items()]) if 0 < len(dict_from_str(feat)) else '_'\n",
    "            for feat in feats\n",
    "        ]\n",
    "        joint_pos_feats = [\n",
    "            f\"{token_upos}#{token_xpos}#{token_joint_feats}\"\n",
    "            for token_upos, token_xpos, token_joint_feats in zip(upos, xpos, joint_feats, strict=True)\n",
    "        ]\n",
    "        \n",
    "    sequence_length = len(words)\n",
    "    \n",
    "    deps_matrix_ud = None\n",
    "    if heads is not None and deprels is not None:\n",
    "        deps_matrix_ud = [[''] * sequence_length for _ in range(sequence_length)]\n",
    "        for index, (head, relation) in enumerate(zip(heads, deprels, strict=True)):\n",
    "            # Skip nulls.\n",
    "            if head == -1:\n",
    "                continue\n",
    "            assert 0 <= head\n",
    "            # Hack: start indexing at 0 and replace ROOT with self-loop.\n",
    "            # It makes parser implementation much easier.\n",
    "            if head == 0:\n",
    "                # Replace ROOT with self-loop.\n",
    "                head = index\n",
    "            else:\n",
    "                # If not ROOT, shift token left.\n",
    "                head -= 1\n",
    "                assert head != index, f\"head = {head + 1} must not be equal to index = {index + 1}\"\n",
    "            deps_matrix_ud[index][head] = relation\n",
    "\n",
    "    deps_matrix_eud = None\n",
    "    if deps is not None:\n",
    "        deps_matrix_eud = [[''] * sequence_length for _ in range(sequence_length)]\n",
    "        for index, dep in enumerate(deps):\n",
    "            dep = dict_from_str(dep) # Convert string representation of dict to a dict.\n",
    "            assert 0 < len(dep), f\"Deps must not be empty\"\n",
    "            for head, relation in dep.items():\n",
    "                assert 0 <= head\n",
    "                # Hack: start indexing at 0 and replace ROOT with self-loop.\n",
    "                # It makes parser implementation much easier.\n",
    "                if head == 0:\n",
    "                    # Replace ROOT with self-loop.\n",
    "                    head = index\n",
    "                else:\n",
    "                    # If not ROOT, shift token left.\n",
    "                    head -= 1\n",
    "                    assert head != index, f\"head = {head + 1} must not be equal to index = {index + 1}\"\n",
    "                deps_matrix_eud[index][head] = relation\n",
    "\n",
    "    return {\n",
    "        \"lemma_rules\": lemma_rules,\n",
    "        \"joint_pos_feats\": joint_pos_feats,\n",
    "        \"deps_ud\": deps_matrix_ud,\n",
    "        \"deps_eud\": deps_matrix_eud\n",
    "    }\n",
    "\n",
    "\n",
    "def update_schema_with_class_labels(dataset_dict: DatasetDict) -> Features:\n",
    "    \"\"\"Update the schema to use ClassLabel for specified columns.\"\"\"\n",
    "\n",
    "    def extract_unique_labels(dataset, column_name, is_matrix=False) -> list[str]:\n",
    "        \"\"\"Extract unique labels from a specific column in the dataset.\"\"\"\n",
    "        if is_matrix:\n",
    "            all_labels = [value for matrices in dataset[column_name] for matrix in matrices for value in matrix]\n",
    "        else:\n",
    "            all_labels = itertools.chain.from_iterable(dataset[column_name])\n",
    "        return sorted(set(all_labels)) # Ensure consistent ordering of labels\n",
    "\n",
    "    # Extract labels from train dataset only, since all the labels must be present in training data.\n",
    "    train_dataset = dataset_dict['train']\n",
    "\n",
    "    # Extract unique labels for each column that needs to be ClassLabel.\n",
    "    lemma_rule_labels = extract_unique_labels(train_dataset, \"lemma_rules\")\n",
    "    joint_pos_feats_labels = extract_unique_labels(train_dataset, \"joint_pos_feats\")\n",
    "    deps_ud_labels = extract_unique_labels(train_dataset, \"deps_ud\", is_matrix=True)\n",
    "    deps_eud_labels = extract_unique_labels(train_dataset, \"deps_eud\", is_matrix=True)\n",
    "    misc_labels = extract_unique_labels(train_dataset, \"miscs\")\n",
    "    deepslot_labels = extract_unique_labels(train_dataset, \"deepslots\")\n",
    "    semclass_labels = extract_unique_labels(train_dataset, \"semclasses\")\n",
    "\n",
    "    # Define updated features schema\n",
    "    features = Features({\n",
    "        \"words\": Sequence(Value(\"string\")),\n",
    "        \"lemma_rules\": Sequence(ClassLabel(names=lemma_rule_labels), ),\n",
    "        \"joint_pos_feats\": Sequence(ClassLabel(names=joint_pos_feats_labels)),\n",
    "        \"deps_ud\": Sequence(Sequence(ClassLabel(names=deps_ud_labels))),\n",
    "        \"deps_eud\": Sequence(Sequence(ClassLabel(names=deps_eud_labels))),\n",
    "        \"miscs\": Sequence(ClassLabel(names=misc_labels)),\n",
    "        \"deepslots\": Sequence(ClassLabel(names=deepslot_labels)),\n",
    "        \"semclasses\": Sequence(ClassLabel(names=semclass_labels)),\n",
    "        \"metadata\": Value(\"string\")\n",
    "    })\n",
    "    return features\n",
    "\n",
    "\n",
    "dataset = create_dataset()\n",
    "dataset = dataset.map(preprocess, remove_columns=['lemmas', 'upos', 'xpos', 'feats', 'heads', 'deprels', 'deps'])\n",
    "\n",
    "class_features = update_schema_with_class_labels(dataset)\n",
    "dataset = dataset.cast(class_features)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = dataset.with_format(\"torch\", device=device)\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80aa089c-953a-49c0-a1e3-f4721ba77a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [['The', 'firm', \"'s\", 'snowmobile', 'division', 'and', 'defence', 'services', 'unit', 'were', 'also', 'sold', 'and', 'Bombardier', 'started', 'the', 'development', 'of', 'a', 'new', 'aircraft', 'seating', '110', 'to', '135', 'passengers', '.'], ['Mr', 'Majumdar', 'also', 'said', 'an', 'assessment', 'should', 'be', 'made', 'as', 'to', 'whether', 'foreign', 'investment', 'is', 'indeed', 'beneficial', 'to', 'the', 'country', '-', 'in', 'terms', 'of', 'employment', 'and', 'money', 'generated', '-', 'or', 'just', 'another', 'way', 'of', 'international', 'companies', 'filling', 'their', 'deep', 'pockets', '.'], ['This', '#NULL', 'means', 'mobile', 'companies', 'have', 'to', 'think', 'carefully', 'about', 'what', 'they', 'are', 'offering', 'in', 'new', 'models', 'so', 'that', 'people', 'see', 'a', 'compelling', 'reason', 'to', 'upgrade', ',', 'said', 'Gartner', '.'], ['The', 'agenda', 'was', 'just', 'too', 'broad', 'and', 'as', 'a', 'result', 'nothing', 'was', 'prioritised', '.']], 'lemma_rules': tensor([[   0,    0,    0,    0,    0,    0,    0,   13,    0,  143,    0,   74,\n",
      "            0,    0,   32,    0,    0,    0,    0,    0,    0,   63,    0,    0,\n",
      "            0,   13,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100],\n",
      "        [   2,    0,    0,   60,    0,    0,    0,    0,   51,    0,    0,    0,\n",
      "            0,    0,   38,    0,    0,    0,    0,    0,    0,    0,   13,    0,\n",
      "            0,    0,    0,   13,    0,    0,    0,    0,    0,    0,    0,   90,\n",
      "           63,    0,    0,   13,    0],\n",
      "        [   0,    0,   13,    0,   90,    0,    0,    0,    0,    0,    0,    0,\n",
      "          147,   63,    0,    0,   13,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   60,    0,    0, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100],\n",
      "        [   0,    0,   69,    0,    0,    0,    0,    0,    0,    0,    0,   69,\n",
      "           13,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100]], device='cuda:0'), 'joint_pos_feats': tensor([[  53,   79,   96,   79,   79,   50,   79,   78,   79,   34,   21,  192,\n",
      "           50,  152,  179,   53,   79,   12,   54,    2,   79,  195,   85,   12,\n",
      "           85,   78,  160, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100],\n",
      "        [ 152,  152,   21,  179,   54,   79,   46,   48,  192,   13,   13,  163,\n",
      "            2,   79,   41,   21,    2,   12,   53,   79,  160,   12,   78,   12,\n",
      "           79,   50,   79,  191,  160,   50,   21,  142,   79,   12,    2,   78,\n",
      "          195,  116,    2,   78,  160],\n",
      "        [ 135,   79,  180,   79,   78,  173,   12,  196,   21,   12,  133,  126,\n",
      "           35,  193,   12,    2,   78,  164,   13,   78,  173,   54,    2,   79,\n",
      "           12,  196,  160,  179,  152,  160, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100],\n",
      "        [  53,   79,   40,   21,   21,    2,   50,  163,   54,   79,  134,   40,\n",
      "          192,  160, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100]], device='cuda:0'), 'deps_ud': tensor([[[   0,   20,    0,  ..., -100, -100, -100],\n",
      "         [   0,    0,    0,  ..., -100, -100, -100],\n",
      "         [   0,   11,    0,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]],\n",
      "\n",
      "        [[   0,    0,    0,  ...,    0,    0,    0],\n",
      "         [  26,    0,    0,  ...,    0,    0,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0],\n",
      "         ...,\n",
      "         [   0,    0,    0,  ...,    0,    7,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[   0,    0,   36,  ..., -100, -100, -100],\n",
      "         [   0,    0,    0,  ..., -100, -100, -100],\n",
      "         [   0,    0,    0,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]],\n",
      "\n",
      "        [[   0,   20,    0,  ..., -100, -100, -100],\n",
      "         [   0,    0,    0,  ..., -100, -100, -100],\n",
      "         [   0,    0,    0,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]]]), 'deps_eud': tensor([[[   0,   55,    0,  ..., -100, -100, -100],\n",
      "         [   0,    0,    0,  ..., -100, -100, -100],\n",
      "         [   0,   42,    0,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]],\n",
      "\n",
      "        [[   0,    0,    0,  ...,    0,    0,    0],\n",
      "         [  61,    0,    0,  ...,    0,    0,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0],\n",
      "         ...,\n",
      "         [   0,    0,    0,  ...,    0,   38,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[   0,   55,    0,  ..., -100, -100, -100],\n",
      "         [   0,    0,  120,  ..., -100, -100, -100],\n",
      "         [   0,    0,    0,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]],\n",
      "\n",
      "        [[   0,   55,    0,  ..., -100, -100, -100],\n",
      "         [   0,    0,    0,  ..., -100, -100, -100],\n",
      "         [   0,    0,    0,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]]]), 'miscs': tensor([[   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100],\n",
      "        [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    1,    2],\n",
      "        [   2,    9,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    1,    2,    2,    1,    2, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100],\n",
      "        [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100]], device='cuda:0'), 'deepslots': tensor([[ 134,  132,  134,   72,   72,  134,   74,  119,   72,  134,    1,   95,\n",
      "          134,    5,   95,  134,   74,  134,  134,  129,   72,   85,  105,  134,\n",
      "          105,   72,  134, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100],\n",
      "        [   5,   71,    1,   95,  134,   72,  134,  134,   74,   14,  134,  134,\n",
      "           52,   72,   32,   14,  122,  134,  134,   39,  134,  134,  121,  134,\n",
      "           40,  134,   40,   85,  134,  134,   86,   14,   40,  134,   14,    5,\n",
      "           23,   90,   52,   54,  134],\n",
      "        [  13,  108,   95,  119,   39,   32,  134,   74,   14,  134,  127,    5,\n",
      "          134,   85,  134,  129,   64,  134,  134,   39,  102,  134,   14,   74,\n",
      "          134,  102,  134,   95,    5,  134, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100],\n",
      "        [ 134,   74,   95,   86,   34,  122,  134,  134,  134,   14,   40,  134,\n",
      "           95,  134, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100]], device='cuda:0'), 'semclasses': tensor([[  18,   96,  541,  520,  249,  113,  126,  521,  249,   20,  140,  416,\n",
      "          113,   96,   23,   18,  435,  272,   18,   78,  520,  110,   78,  272,\n",
      "           78,   25,  541, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100],\n",
      "        [  25,   25,  140,  531,   18,  208,   20,   20,  384,   66,  541,  105,\n",
      "          117,  214,   22,  212,   43,  272,   18,  117,  541,  272,   42,  272,\n",
      "          310,  113,  214,  414,  541,  113,  140,   66,  209,  272,  117,   96,\n",
      "          268,   96,   54,  244,  541],\n",
      "        [  78,  158,  437,   14,   96,  212,  241,  507,   57,  272,  158,   96,\n",
      "           20,  463,  272,   78,  190,  105,  541,  179,  445,   18,  322,   90,\n",
      "          241,  495,  541,  531,   25,  541, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100],\n",
      "        [  18,  302,   22,  140,   52,   85,  113,  105,   18,  296,  158,   20,\n",
      "          482,  541, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100]], device='cuda:0'), 'metadata': ['{\\'sent_id\\': \\'2049\\', \\'text\\': \"The firm\\'s snowmobile division and defence services unit were also sold and Bombardier started the development of a new aircraft seating 110 to 135 passengers.\"}', \"{'sent_id': '6145', 'text': 'Mr Majumdar also said an assessment should be made as to whether foreign investment is indeed beneficial to the country - in terms of employment and money generated - or just another way of international companies filling their deep pockets.'}\", \"{'sent_id': '4099', 'text': 'This means mobile companies have to think carefully about what they are offering in new models so that people see a compelling reason to upgrade, said Gartner.'}\", \"{'sent_id': '4100', 'text': 'The agenda was just too broad and as a result nothing was prioritised.'}\"]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# utils.py\n",
    "def pad_matrices(matrices: Tensor, padding_value: int = -1) -> Tensor:\n",
    "    # Determine the maximum size in each dimension\n",
    "    max_height = max(t.size(0) for t in matrices)\n",
    "    max_width = max(t.size(1) for t in matrices)\n",
    "    assert max_height == max_width, \"UD and E-UD matrices must be square.\"\n",
    "    \n",
    "    # Create a single tensor for stacking with padding\n",
    "    # Initialize with -1 and then copy the tensors into it\n",
    "    padded_tensor = torch.full((len(matrices), max_height, max_width), padding_value)\n",
    "    \n",
    "    # Stack tensors directly into the larger tensor\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        padded_tensor[i, :matrix.size(0), :matrix.size(1)] = matrix\n",
    "    return padded_tensor\n",
    "\n",
    "\n",
    "# data.py\n",
    "def collate_fn(batches: list[dict[str, list | Tensor]]) -> dict[str, list | Tensor]:\n",
    "    padding_value = -1\n",
    "    stack_list_column = lambda column: [batch[column] for batch in batches]\n",
    "    pad_sequence_column = lambda column: pad_sequence([batch[column] for batch in batches], padding_value=padding_value, batch_first=True)\n",
    "    pad_matrix_column = lambda column: pad_matrices([batch[column] for batch in batches], padding_value=padding_value)\n",
    "    return {\n",
    "        \"words\": stack_list_column('words'),\n",
    "        \"lemma_rules\": pad_sequence_column('lemma_rules'),\n",
    "        \"joint_pos_feats\": pad_sequence_column('joint_pos_feats'),\n",
    "        \"deps_ud\": pad_matrix_column('deps_ud'),\n",
    "        \"deps_eud\": pad_matrix_column('deps_eud'),\n",
    "        \"miscs\": pad_sequence_column('miscs'),\n",
    "        \"deepslots\": pad_sequence_column('deepslots'),\n",
    "        \"semclasses\": pad_sequence_column('semclasses'),\n",
    "        \"metadata\": stack_list_column('metadata')\n",
    "    }\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset['train'], batch_size=4, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05d399-8008-4665-bcaf-e03f9bcdd9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
