{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0974f8a0-b248-4362-9815-abd6fac1f683",
   "metadata": {},
   "source": [
    "## Batchnorm\n",
    "\n",
    "Применяет нормализацию по батчу; подробнее описано в [этой статье](https://arxiv.org/abs/1502.03167). Формула при этом используется следующая: $$x = \\frac{x - E[x]}{\\sqrt{var[x] + \\epsilon}} * \\gamma + \\beta$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9741ba9-50fb-45ae-acdd-6417d666c79e",
   "metadata": {},
   "source": [
    "Среднее значение и стандартное отклонение рассчитываются для каждого измерения по мини-батчу, $\\gamma$ и $\\beta$ - обучаемые вектора параметров размера С (где С - размер инпута). По умолчанию элементы $\\gamma$ устанавливаются равными единице, а элементы $\\beta$ устанавливаются равными нулю. Стандартное отклонение вычисляется с помощью смещенной оценки, эквивалентной torch.var(input, unbiased=False).\n",
    "\n",
    "Получается, что при этом мы нормализуем все входные данные к интервалу N~(0,1). Это может спасти нас от затухающих градиентов, если мы используем сигмоиду в качестве активации, потому что:\n",
    "\n",
    "<img src=\"img/batchnorm.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00926f40-98ff-4c1c-9700-7aea915546e0",
   "metadata": {},
   "source": [
    "Чаще всего нормализация по батчу нужна для обучения сверточных нейронных сетей (CNN) и работы с картинками. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3bf70b-990a-473c-8340-e3b581c500cf",
   "metadata": {},
   "source": [
    "В этом простеньком примере посмотрим, как использовать нормализацию. Возьмем стандартный датасет [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html): в этом датасете много маленьких картинок; всего у картинок 10 классов, по 6 тысяч картинок на класс. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb5d5c-dd3c-46b6-97f8-d9e983839381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10 # датасет есть в учебных датасетах torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms # здесь есть полезные инструменты для обработки картинок - нам понадобится функция для превращения картинки в тензор"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3bea9-c008-4672-9f01-1d8245841b9f",
   "metadata": {},
   "source": [
    "Напишем самую простую архитектуру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3095bd-255f-45e1-85e1-1e171ed57534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Многослойный перцептрон.\n",
    "    На входе: 32 * 32 * 3 (картинки 32х32 пикселя, 3 канала)\n",
    "    Скрытый слой: 64 нейрона\n",
    "    Второй скрытый слой: 32 нейрона (нормальная практика - делать на\n",
    "    втором слое меньше в два раза)\n",
    "    Выходной слой: 10 нейронов\n",
    "    Поместим два слоя нормализации\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(32 * 32 * 3, 64),\n",
    "      nn.BatchNorm1d(64), # должно быть указано, сколько выходов у нейронов\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.BatchNorm1d(32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8a0f4-2655-41c5-9403-f03b433c4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зафиксируем рандом\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Приготовим датасет\n",
    "dataset = CIFAR10(os.getcwd(), download=True, transform=transforms.ToTensor()) # датасет стандартный - у него свой класс\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
    "\n",
    "# Инициализируем модель\n",
    "mlp = MLP()\n",
    "\n",
    "# выберем функцию потерь и вид градиентного спуска\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "# запускаем трейнлуп\n",
    "for epoch in range(0, 5): # 5 эпох\n",
    "\n",
    "  # печатаем эпоху\n",
    "  print(f'Starting epoch {epoch + 1}')\n",
    "\n",
    "  # текущий лосс обнуляем на каждой эпохе, чтобы его аккумулировать\n",
    "  current_loss = 0.0\n",
    "\n",
    "  # итерируемся по лоадеру\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "    # достаем фичи и цп из батча\n",
    "    inputs, targets = data\n",
    "\n",
    "    # обнуляем градиенты\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # делаем форвард-проброс\n",
    "    outputs = mlp(inputs)\n",
    "\n",
    "    # вычисляем потери\n",
    "    loss = loss_function(outputs, targets)\n",
    "\n",
    "    # делаем бэкпроп\n",
    "    loss.backward()\n",
    "\n",
    "    # делаем оптимизацию весов\n",
    "    optimizer.step()\n",
    "\n",
    "    # печатаем стату\n",
    "    current_loss += loss.item()\n",
    "    if i % 500 == 499: # на последнем батче\n",
    "        print(f'Loss after mini-batch {i + 1}: {current_loss / 500:.3f}')\n",
    "        current_loss = 0.0\n",
    "\n",
    "# обучились\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1bebb-1522-4aa9-8b84-82cc4b54b1d4",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout — это метод машинного обучения, при котором мы удаляем (или «выбрасываем») блоки нейронной сети для имитации одновременного обучения большого количества архитектур. Важно отметить, что отсев может значительно снизить вероятность переобучения во время тренировки.\n",
    "\n",
    "Примерно так выглядит дропаут на схеме:\n",
    "\n",
    "<img src=\"https://api.wandb.ai/files/authors/images/projects/81595/2d131f38.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14edf931-7038-475a-a2f1-4704c4e59b5c",
   "metadata": {},
   "source": [
    "Добавить дропаут в свою архитектуру очень просто:\n",
    "\n",
    "    self.dropout = nn.Dropout(0.25) \n",
    "    \n",
    "(Добавит слой дропаута с p = 0.25). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1dce1f-a79e-446c-9241-86e918cbeccb",
   "metadata": {},
   "source": [
    "Поиграемся с датасетом [Sonar](http://archive.ics.uci.edu/dataset/151/connectionist+bench+sonar+mines+vs+rocks), в котором содержатся данные от сигналов гидролокатора (если я правильно понимаю)), отражающихся от разных камней. Скачаем его и переименуем в csv для удобства (сами данные - просто таблица csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406f9b5-9841-4020-879b-ac9a5e3d2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0605a4a4-9413-40a0-8e00-9986f0528b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.rename('sonar.all-data', 'sonar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372df5f-bf63-4cbc-87d8-b497e9ecaabf",
   "metadata": {},
   "source": [
    "Напишем простенький бейзлайн. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54154f-47d1-4dbc-a200-db2b6d3cd0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081778a5-60c0-4221-b2fd-fa9bf1ef6251",
   "metadata": {},
   "source": [
    "Считаем датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da7ea8-5b12-43bc-9bad-4b7583519b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# собственно считываем\n",
    "data = pd.read_csv(\"sonar.csv\", header=None)\n",
    "X = data.iloc[:, 0:60]\n",
    "y = data.iloc[:, 60]\n",
    "\n",
    "# закодируем ytrue (в исходном датасете буковки)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "\n",
    "# сконвертируем в тензора\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6bb0c7-5293-4548-87bb-2012acb1a265",
   "metadata": {},
   "source": [
    "Определим архитектуру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be05c0-832e-405c-8d12-23b5b761f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SonarModel(nn.Module):\n",
    "    '''\n",
    "    Обычный MLP. На входе 60 признаков\n",
    "    Потом линейный слой из 60 нейронов\n",
    "    Потом 30 нейронов\n",
    "    Потом выходной на один нейрон. У нас бинарная классификация:\n",
    "    R - камень или M - металл\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(60, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(60, 30)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(30, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42747b1c-4599-4b77-84dc-1be2872f7a7b",
   "metadata": {},
   "source": [
    "Напишем трейнлуп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e80adb-1a83-4c78-82e4-5c46d1d661f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, X_train, y_train, X_val, y_val,\n",
    "                n_epochs=300, batch_size=16):\n",
    "    loss_fn = nn.BCELoss() # для бинарной классификации\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
    "    # даже не будем писать dataloader: сделаем ручками\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for start in batch_start:\n",
    "            X_batch = X_train[start:start + batch_size]\n",
    "            y_batch = y_train[start:start + batch_size]\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # валидация\n",
    "    model.eval()\n",
    "    y_pred = model(X_val)\n",
    "    acc = (y_pred.round() == y_val).float().mean()\n",
    "    acc = float(acc)\n",
    "    return acc\n",
    "\n",
    "# 10-фолдовая кросс-валидация: моделька у нас супер-маленькая и датасет тоже, можем позволить\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "accuracies = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    # создаем модель, учим ее и вычисляем Accuracy\n",
    "    torch.manual_seed(42)\n",
    "    model = SonarModel()\n",
    "    acc = model_train(model, X[train], y[train], X[test], y[test])\n",
    "    print(f\"Accuracy: {acc:.2f}\")\n",
    "    accuracies.append(acc)\n",
    "\n",
    "# оцениваем, что получилось в среднем\n",
    "mean = np.mean(accuracies)\n",
    "std = np.std(accuracies)\n",
    "print(f\"Baseline: {mean * 100:.2f} (+/- {std * 100:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee4a3f-7eb2-423d-b39e-6bbf75c6c8fc",
   "metadata": {},
   "source": [
    "Неплохо, но не идеально."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150da90-c44c-4ae7-b9ec-1c5a6fc9d52b",
   "metadata": {},
   "source": [
    "Добавим дропаут на инпуты (иногда так делается):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46987c-fdd3-4419-962d-562cad9e1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SonarModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.2) # 20% шанс отрубить какую-то фичу\n",
    "        self.layer1 = nn.Linear(60, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(60, 30)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(30, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abdb9cb-0012-43be-89ac-00f62c4e53fa",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a65d54-8516-4cef-8dc4-3690fca5cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "accuracies = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    # создаем модель, учим ее и вычисляем Accuracy\n",
    "    torch.manual_seed(42)\n",
    "    model = SonarModel()\n",
    "    acc = model_train(model, X[train], y[train], X[test], y[test])\n",
    "    print(f\"Accuracy: {acc:.2f}\")\n",
    "    accuracies.append(acc)\n",
    "\n",
    "# оцениваем, что получилось в среднем\n",
    "mean = np.mean(accuracies)\n",
    "std = np.std(accuracies)\n",
    "print(f\"Baseline: {mean * 100:.2f} (+/- {std * 100:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afd255-3f56-4184-a56d-d61684cce9c7",
   "metadata": {},
   "source": [
    "Чет не оче. Давайте добавим дропауты на скрытые слои (так делается куда чаще):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f0686-03c3-4439-8f49-d99bf49559f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SonarModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(60, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2) # один после первого\n",
    "        self.layer2 = nn.Linear(60, 30)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2) # один после второго\n",
    "        self.output = nn.Linear(30, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219a91d-6faf-4942-997d-918d43010c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "accuracies = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    # создаем модель, учим ее и вычисляем Accuracy\n",
    "    torch.manual_seed(42)\n",
    "    model = SonarModel()\n",
    "    acc = model_train(model, X[train], y[train], X[test], y[test])\n",
    "    print(f\"Accuracy: {acc:.2f}\")\n",
    "    accuracies.append(acc)\n",
    "\n",
    "# оцениваем, что получилось в среднем\n",
    "mean = np.mean(accuracies)\n",
    "std = np.std(accuracies)\n",
    "print(f\"Baseline: {mean * 100:.2f} (+/- {std * 100:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17304d48-31b0-47da-98b8-325dd17992a1",
   "metadata": {},
   "source": [
    "В зависимости от рандома может оказаться одинаково с бейзлайном, но чаще покажет небольшое улучшение."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlearn",
   "language": "python",
   "name": "mlearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
