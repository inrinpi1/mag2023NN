{"cells":[{"cell_type":"markdown","metadata":{"id":"cf9HpZyXXj7-"},"source":["## __Tensors__"]},{"cell_type":"markdown","metadata":{"id":"z-pqXfQuXj8B"},"source":["Одно из основных понятий в PyTorch -- это __Tensor__.\n","\n","https://pytorch.org/docs/master/tensors.html\n","\n","__Tensor__ -- это такой же массив, как и в __numpy.array__, размерность и тип данных которого мы можем задать. Tensor в отличие от numpy.array может вычисляться на __GPU__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y75beN7iXj8B"},"outputs":[],"source":["import numpy as np\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCD7pY9mXj8C"},"outputs":[],"source":["N = 100\n","D_in = 50\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n","\n","x = np.random.randn(N, D_in)\n","x_torch = torch.randn(N, D_in, device=device, dtype=dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzJDJFVOXj8C"},"outputs":[],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrayyNmIXj8C"},"outputs":[],"source":["x_torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNZ1H8dsXj8D"},"outputs":[],"source":["x_torch = torch.Tensor(np.ones((N, D_in)))\n","x_torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gL1yKAgZXj8D"},"outputs":[],"source":["x_torch = torch.FloatTensor([1, 2, 3])\n","x_torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LzkcFD09Xj8D"},"outputs":[],"source":["x1 = torch.IntTensor([1, 2, 3])\n","x2 = torch.FloatTensor([3, 4, 5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tl9rQhBZXj8D"},"outputs":[],"source":["print(x1)\n","print(x2)"]},{"cell_type":"markdown","metadata":{"id":"aSxr9AwAXj8D"},"source":["В PyTorch можно найти много операций, которые похожи на то, что есть в numpy :\n","```\n","- torch.add (np.add) -> сложение тензоров (поэлементное)\n","- torch.sub (np.subtract) -> вычитание (поэлементное)\n","- torch.mul (np.multiply) -> умнажение скаляров / матриц (поэлементное)\n","- torch.mm (np.matmul) -> перемножение матриц\n","- torch.ones (np.ones) -> создание тензора из единиц\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qowG5J_iXj8E"},"outputs":[],"source":["# Давайте попробуем вышепересчисленные операции"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9kS0aONXj8E"},"outputs":[],"source":["x1 = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gvd7e8iXj8E"},"outputs":[],"source":["x2 = torch.FloatTensor([[7, 8], [9, 1], [2, 3]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjlPulYgXj8E"},"outputs":[],"source":["out = torch.mm(x1, x2)\n","out"]},{"cell_type":"markdown","metadata":{"id":"69pRDhucXj8E"},"source":["```\n","- torch.view (np.reshape) -> изменения порядка элементов в тензоре, не путать с транспонированием.\n","```"]},{"cell_type":"markdown","metadata":{"id":"sY6V1_8VXj8E"},"source":["## Dynamic Computational Graph"]},{"cell_type":"markdown","metadata":{"id":"giYIq9A6Xj8E"},"source":["После того, как были реализованы архитектура модели и весь процес обучения и валидация сети, при запуске кода в PyTorch происходят следующие этапы:"]},{"cell_type":"markdown","metadata":{"id":"XASsPpz5Xj8E"},"source":["1. Строится вычислительный граф (направленный ациклический граф), где каждое ребро, ведущее к другому узлу, -- это тензор, а узел - это выполнение операции над данным тензором."]},{"cell_type":"markdown","metadata":{"id":"wVUMmvhPXj8F"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/Graph.png?raw=1\" alt=\"Drawing\" style=\"width: 300px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"z__2ATORXj8F"},"source":["Реализуем двухслойную сеть для задачи регрессии. И граф для такой архитектуры будет выглядеть следующим образом:"]},{"cell_type":"markdown","metadata":{"id":"zIXIg1ZHXj8F"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/RegGraph.png?raw=1\" alt=\"Drawing\" />"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfENUXu4Xj8F"},"outputs":[],"source":["batch_size = 64\n","input_size = 3\n","hidden_size = 2\n","output_size = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvJoF6UKXj8F","executionInfo":{"status":"ok","timestamp":1695730565771,"user_tz":-180,"elapsed":289,"user":{"displayName":"","userId":""}},"outputId":"7db17749-8a97-4fcd-a7ff-1176c31c7a5b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss on iteration 99 = 171.24261474609375\n","Loss on iteration 199 = 163.24754333496094\n","Loss on iteration 299 = 155.96214294433594\n","Loss on iteration 399 = 149.30804443359375\n","Loss on iteration 499 = 143.21759033203125\n"]}],"source":["# Create random input and output data\n","x = torch.randn(batch_size, input_size, device=device, dtype=dtype)\n","y = torch.randn(batch_size, output_size, device=device, dtype=dtype)\n","\n","# Randomly initialize weights\n","w1 = torch.randn(input_size, hidden_size, device=device, dtype=dtype)\n","w2 = torch.randn(hidden_size, output_size, device=device, dtype=dtype)\n","\n","learning_rate = 1e-6\n","for t in range(500):\n","    # Forward pass: compute predicted y\n","    #TODO\n","    h_1 = x.mm(w1)\n","    h_relu = h_1.clamp(min=0)\n","    out = h_relu.mm(w2)\n","\n","    # Compute and print loss\n","    loss = (out - y).pow(2).sum().item()\n","\n","    # Backward pass:\n","    dloss_dout = 2 * (out - y)\n","  # dl/do * h1\n","    grad_w2 = h_relu.t().mm(dloss_dout)\n","\n","    grad_h_relu = dloss_dout.mm(w2.t())\n","    grad_h_relu[h_1 < 0] = 0\n","\n","    grad_w1 = x.t().mm(grad_h_relu)\n","\n","    w1 -= learning_rate * grad_w1\n","    w2 -= learning_rate * grad_w2\n","    b1 -= learning_rate * grad_b1.sum()\n","    b2 -= learning_rate * grad_b2.sum()\n","    if t % 100 == 99:\n","        print(f'Loss on iteration {t} = {loss}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EPv2vqYXj8F"},"outputs":[],"source":["loss"]},{"cell_type":"markdown","metadata":{"id":"PQplLFE1Xj8F"},"source":["## Autograd"]},{"cell_type":"markdown","metadata":{"id":"AdGMfCZJXj8F"},"source":["2. Еще одно фундаментальное понятие и важный элемент при построении графа -- это __Autograd__ -- автоматическое дифференцирование.\n","\n","Для того чтобы с помощью стохастического градиентного спуска обновить обучаемые параметры сети, нужно посчитать градиенты. И как известно, обновление весов, которые участвуют в нескольких операциях, происходит по `правилу дифференцирования сложной функции` (цепное правило или __chain rule__)."]},{"cell_type":"markdown","metadata":{"id":"OliJ27L7Xj8G"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/RegChainRule.png?raw=1\" alt=\"Drawing\" />"]},{"cell_type":"markdown","metadata":{"id":"NwhBGzXuXj8G"},"source":["То есть (1) вычислительный граф позволяет определить последовательность операций, а (2) автоматическое дифференцирование посчитать нужные градиенты."]},{"cell_type":"markdown","metadata":{"id":"PR_tNTwxXj8G"},"source":["Если бы `Autograd` не было, то тогда backprop надо было бы реализовывать самим, и как это бы выглядело?"]},{"cell_type":"markdown","metadata":{"id":"rAvhPhUYXj8G"},"source":["Рассмотрим на примере, как посчиать градиенты для весов из входного слоя, где входной вектора `X` состоит из 3-х компонент. А входной слой вторую размерность имеет равной 2.\n","\n","После чего это идет в `ReLU`, но для простоты опустим на время ее, и посмотрим как дальше это идет по сети.\n","\n","Ниже написано, как это все вычисляется и приводит нас к значению целевой функции для одного наблюдения"]},{"cell_type":"markdown","metadata":{"id":"r0t5J13cXj8G"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/1.png?raw=1\" alt=\"1\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"cPfxn59KXj8G"},"source":["Тогда, чтобы посчитать градиент по первому элементу из обучаемой матрицы на первом слое, необходимо взять производную у сложной функции. А этот как раз делается по `chain rule`: сначала берем у внешней, потом спускаемся на уровень ниже, и так пока не дойдем до той функции, после которой эта переменная уже нигде не участвует:"]},{"cell_type":"markdown","metadata":{"id":"Vd8CbUioXj8H"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/2.png?raw=1\" alt=\"2\" style=\"width: 400px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"sFVnVY6JXj8H"},"source":["Перепишем это все в матричном виде, то есть сделаем аналог вида матрицы весов из первого слоя, но там уже будут её градиенты, котоыре будут нужны чтобы как раз обновить эти веса:"]},{"cell_type":"markdown","metadata":{"id":"A4Ve0bBwXj8H"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/3.jpg?raw=1\" alt=\"3\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"jkfyuAmqXj8H"},"source":["Как видно, здесь можно вектор X вынести, то есть разделить на две матрицы:"]},{"cell_type":"markdown","metadata":{"id":"FCih8eFFXj8H"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/4.jpg?raw=1\" alt=\"4\" style=\"width: 500px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"V9y9AhtaXj8H"},"source":["То есть уже видно, что будем траспонировать входной вектор(матрицу). Но надо понимать, что в реальности у нас не одно наблюдение в батче, а несколько, тогда запись немного изменит свой вид:"]},{"cell_type":"markdown","metadata":{"id":"NuGrsCalXj8N"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/5.jpg?raw=1\" alt=\"5\" style=\"width: 500px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"nqVgjOUFXj8N"},"source":["Теперь мы видим, как на самом деле вычисляется вот те самые частные производные для вектора X, то есть видно, как математически это можно записать, а именно:"]},{"cell_type":"markdown","metadata":{"id":"F4dmL9LmXj8N"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/6.jpg?raw=1\" alt=\"6\" style=\"width: 500px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"GAtEKdAmXj8N"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/7.jpg?raw=1\" alt=\"7\" style=\"width: 500px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"bHWyFEIpXj8N"},"source":["Уже можно реализовать. Понятно, что транспонируется, что нет, и что на что умножается."]},{"cell_type":"markdown","metadata":{"id":"4Kdb-o66Xj8N"},"source":["Но помним про ReLU. Для простоты опустили, но теперь её учесть будет легче.\n","\n","Так как после первого слоя идет ReLU, а значит, занулились те выходы первого слоя, которые были __меньше__ нуля. Получается, что во второй слой не все дошло, тогда нужно обнулить, что занулил ReLU.\n","\n","Что занулил ReLU, мы можем выяснить при `forward pass`, а где именно поставить нули, то надо уже смотреть относительно `backward propagation`, на том выходе, где последний раз участвовал выход после ReLU, то есть:"]},{"cell_type":"markdown","metadata":{"id":"eBDc1MvTXj8N"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/8.jpg?raw=1\" alt=\"8\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"-wXX3KxxXj8N"},"source":["Благодаря `Autograd` реализацию `chain rule` можно избежать, так как для более сложных нейронных сетей вручную такое реализовать сложно, при этом сделать это эффективным."]},{"cell_type":"markdown","metadata":{"id":"dYXJpfEiXj8O"},"source":["Для того чтобы PyTorch понял, за какими переменными надо \"следить\", то есть указать, что именно \"эти\" переменные являются обучаемыми, необходимо при создании тензора в качестве аттрибута указать __requires_grad=True__:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ju85VBFXj8O"},"outputs":[],"source":["w1 = torch.randn(input_size, hidden_size, device=device, dtype=dtype, requires_grad=True)\n","w2 = torch.randn(hidden_size, output_size, device=device, dtype=dtype, requires_grad=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PyJffH2jXj8O"},"outputs":[],"source":["learning_rate = 1e-6\n","for t in range(500):\n","    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n","\n","    loss = (y_pred - y).pow(2).sum()\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    # Теперь подсчет градиентов для весов происходит при вызове backward\n","    loss.backward()\n","\n","    # Обновляем значение весов, но укзаываем, чтобы PyTorch не считал эту операцию,\n","    # которая бы учавствовала бы при подсчете градиентов в chain rule\n","    with torch.no_grad():\n","        w1 -= learning_rate * w1.grad\n","        w2 -= learning_rate * w2.grad\n","\n","        # Теперь обнуляем значение градиентов, чтобы на следующем шаге\n","        # они не учитывались при подсчете новых градиентов,\n","        # иначе произойдет суммирвоание старых и новых градиентов\n","        w1.grad.zero_()\n","        w2.grad.zero_()"]},{"cell_type":"markdown","metadata":{"id":"vsp06h3IXj8O"},"source":["Осталось еще не вручную обновлять веса, а использовать адаптивные методы градиентного спуска. Для этого нужно использовать модуль __optim__. А помимо оптимайзера, еще можно использовать готовые целевые функции из модлуя __nn__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-C8vw38MXj8O"},"outputs":[],"source":["w1 = torch.randn(input_size, hidden_size, device=device, dtype=dtype, requires_grad=True)\n","w2 = torch.randn(hidden_size, output_size, device=device, dtype=dtype, requires_grad=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn1K-p64Xj8O"},"outputs":[],"source":["import torch.optim as optim\n","\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","\n","learning_rate = 1e-6\n","optimizer = torch.optim.Adam([w1, w2], lr=learning_rate)\n","\n","for t in range(500):\n","    optimizer.zero_grad()\n","\n","    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n","\n","    loss = loss_fn(y_pred, y)\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    loss.backward()\n","\n","    optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"hcWbCWLNXj8O"},"source":["После того, как мы сделали backward, в этот момент посчитались градиенты и граф уничтожился, то есть стёрлись все пути, которые связывали тензоры между собой. Это значит, что еще раз backward сделать не поулчится, будет ошибка. Но если вдруг нужно считать градиенты еще раз, то нужно при вызове backward задать `retain_graph=True`."]},{"cell_type":"markdown","metadata":{"id":"RKtR96b5Xj8P"},"source":["Еще важный аттрибут, который есть у Tensor -- это `grad_fn`. В этом аттрибуте указывается та функция, посредством которой был создан этот тензор. Так PyTorch понимает, как именно считать по нему градиент."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3m0hgrMXj8P"},"outputs":[],"source":["y_pred.grad_fn"]},{"cell_type":"markdown","metadata":{"id":"ikzgolzmXj8P"},"source":["Также можно контролировать, должны ли градиенты течь или нет."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4G1eT9dXj8P"},"outputs":[],"source":["x = torch.tensor([1.], requires_grad=True)\n","with torch.no_grad():\n","    with torch.enable_grad():\n","        y = x * 2\n","y.requires_grad"]},{"cell_type":"markdown","metadata":{"id":"NGypIryFXj8P"},"source":["## Почему Backprop надо понимать"]},{"cell_type":"markdown","metadata":{"id":"0-UVqZpOXj8P"},"source":["1. Backprop позволяет понимать, как те или иные операции, сложные конструкции в сети влияют на обнолвение весов.\n","Почему лучше сделать конкатенацию тензоров, а не поэлементное сложение. Для этого нужно посмотреть на backprop, как будут обновляться веса.\n","\n","2. Даже на таком маленьком примере двуслойной MLP можно уже увидеть, когда `ReLU`, как функцию активации, не очень хорошо применять. Если разреженные данные, то получить на выходе много нулей вероятнее, чем при использовании `LeakyReLU`, то есть, градиенты будут нулевыми и веса никак не будут обновляться => сеть не обучается!\n","\n","3. В архитектуре могут встречаться недифференцируемые операции, и первое - это нужно понять, потому что при обучении сети это может быть не сразу заметно, просто качество модели будет плохое, и не получится достичь хорошей точности.\n","\n","Например, в одной из статей было предложено в качестве механизма внимания применить распределение Бернулли, которое умножается на выход промежуточного слоя сети. И эта операция недифференцируема, нужно реализовывать backprop самим, тем самым обеспечить корректное протекание градиентов.\n"]},{"cell_type":"markdown","metadata":{"id":"wgZpA1sOXj8P"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/Bernoulli.png?raw=1\" alt=\"8\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"h6C-mELFXj8Q"},"source":["Так же в любой статье, которая предлагает новую целевую функцию для той или иной задачи, всегда будут представлены градиенты, чтобы было понимание, как это влияет на обновление весов. Не просто так !"]},{"cell_type":"markdown","metadata":{"id":"u0v9iUS0Xj8Q"},"source":["<img src=\"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/images/BernoulliBackProp.png?raw=1\" alt=\"8\" style=\"width: 600px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"81njs7MeXj8Q"},"source":["## nn.Module"]},{"cell_type":"markdown","metadata":{"id":"bMH8kDjbXj8Q"},"source":["В предыдущем примере архитектуру сети создавали используя последовательной способ объявления слоев сети -- `nn.Sequential`.\n","\n","Но еще можно это сделать более гибким подходом:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1psLEhFXj8Q"},"outputs":[],"source":["import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Zk-YSfCXj8Q"},"outputs":[],"source":["class TwoLayerNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        \"\"\"\n","        TwoLayerNet наследуется от nn.Module и тем самым получаем возможность\n","        переопределять методы класса.\n","        В конструкторе создаем слои (обучаемые веса) и другие нужные переменные/функции,\n","        которые нужны для модели\n","        \"\"\"\n","        super(TwoLayerNet, self).__init__()\n","        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n","        self.linear2 = torch.nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Метод forward отвечает за прямое распространение модели,\n","        поэтому данный метод нужно переопределять обязательно,\n","        чтобы задать логику прямого распространения.\n","        Именно в этот момент начинает строиться динамический граф\n","        \"\"\"\n","        h_relu = self.linear1(x).clamp(min=0)\n","        y_pred = self.linear2(h_relu)\n","\n","        return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQ43VNmLXj8Q"},"outputs":[],"source":["batch_size = 64\n","input_size = 1000\n","hidden_size = 100\n","output_size = 10\n","\n","x = torch.randn(batch_size, input_size, device=device, dtype=dtype)\n","y = torch.randn(batch_size, output_size, device=device, dtype=dtype)\n","\n","model = TwoLayerNet(input_size, hidden_size, output_size)\n","\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n","\n","for t in range(500):\n","    y_pred = model(x)\n","\n","    loss = loss_fn(y_pred, y)\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[{"file_id":"https://github.com/rsuh-python/mag2022/blob/main/CL/term03NN/01-MLP/sem01_autograd.ipynb","timestamp":1695730853480}]}},"nbformat":4,"nbformat_minor":0}